{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bsundar937/GAN_stability/blob/main/MNIST_Bimodal_GANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Original architecture implemented for MNIST images**"
      ],
      "metadata": {
        "id": "v4gYW61XvqsJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXVvC9DL4Fis",
        "collapsed": true,
        "outputId": "6bd0b69d-1fdd-47b7-b965-aeceae731991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 106306185.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to ../../data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 69180864.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ../../data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 27378175.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ../../data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4246662.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../../data/mnist/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0/1] [Batch 0/938] [D loss: 0.670900] [G loss: 0.726798]\n",
            "[Epoch 0/1] [Batch 1/938] [D loss: 0.581541] [G loss: 0.723864]\n",
            "[Epoch 0/1] [Batch 2/938] [D loss: 0.509984] [G loss: 0.720876]\n",
            "[Epoch 0/1] [Batch 3/938] [D loss: 0.455439] [G loss: 0.717954]\n",
            "[Epoch 0/1] [Batch 4/938] [D loss: 0.414276] [G loss: 0.714317]\n",
            "[Epoch 0/1] [Batch 5/938] [D loss: 0.387793] [G loss: 0.709136]\n",
            "[Epoch 0/1] [Batch 6/938] [D loss: 0.374933] [G loss: 0.703517]\n",
            "[Epoch 0/1] [Batch 7/938] [D loss: 0.366462] [G loss: 0.696953]\n",
            "[Epoch 0/1] [Batch 8/938] [D loss: 0.365518] [G loss: 0.688109]\n",
            "[Epoch 0/1] [Batch 9/938] [D loss: 0.367235] [G loss: 0.678489]\n",
            "[Epoch 0/1] [Batch 10/938] [D loss: 0.370662] [G loss: 0.665841]\n",
            "[Epoch 0/1] [Batch 11/938] [D loss: 0.377260] [G loss: 0.652373]\n",
            "[Epoch 0/1] [Batch 12/938] [D loss: 0.380374] [G loss: 0.644507]\n",
            "[Epoch 0/1] [Batch 13/938] [D loss: 0.388511] [G loss: 0.631715]\n",
            "[Epoch 0/1] [Batch 14/938] [D loss: 0.392587] [G loss: 0.624561]\n",
            "[Epoch 0/1] [Batch 15/938] [D loss: 0.395026] [G loss: 0.623017]\n",
            "[Epoch 0/1] [Batch 16/938] [D loss: 0.397588] [G loss: 0.622542]\n",
            "[Epoch 0/1] [Batch 17/938] [D loss: 0.400299] [G loss: 0.621094]\n",
            "[Epoch 0/1] [Batch 18/938] [D loss: 0.399037] [G loss: 0.631627]\n",
            "[Epoch 0/1] [Batch 19/938] [D loss: 0.400696] [G loss: 0.637191]\n",
            "[Epoch 0/1] [Batch 20/938] [D loss: 0.401294] [G loss: 0.645887]\n",
            "[Epoch 0/1] [Batch 21/938] [D loss: 0.400680] [G loss: 0.659503]\n",
            "[Epoch 0/1] [Batch 22/938] [D loss: 0.400276] [G loss: 0.660547]\n",
            "[Epoch 0/1] [Batch 23/938] [D loss: 0.399148] [G loss: 0.665651]\n",
            "[Epoch 0/1] [Batch 24/938] [D loss: 0.403459] [G loss: 0.669918]\n",
            "[Epoch 0/1] [Batch 25/938] [D loss: 0.409385] [G loss: 0.664826]\n",
            "[Epoch 0/1] [Batch 26/938] [D loss: 0.405850] [G loss: 0.678596]\n",
            "[Epoch 0/1] [Batch 27/938] [D loss: 0.411915] [G loss: 0.660223]\n",
            "[Epoch 0/1] [Batch 28/938] [D loss: 0.424317] [G loss: 0.669181]\n",
            "[Epoch 0/1] [Batch 29/938] [D loss: 0.433238] [G loss: 0.644327]\n",
            "[Epoch 0/1] [Batch 30/938] [D loss: 0.449645] [G loss: 0.629052]\n",
            "[Epoch 0/1] [Batch 31/938] [D loss: 0.448615] [G loss: 0.654644]\n",
            "[Epoch 0/1] [Batch 32/938] [D loss: 0.453017] [G loss: 0.614229]\n",
            "[Epoch 0/1] [Batch 33/938] [D loss: 0.463065] [G loss: 0.638059]\n",
            "[Epoch 0/1] [Batch 34/938] [D loss: 0.466602] [G loss: 0.612047]\n",
            "[Epoch 0/1] [Batch 35/938] [D loss: 0.471505] [G loss: 0.662265]\n",
            "[Epoch 0/1] [Batch 36/938] [D loss: 0.469872] [G loss: 0.608539]\n",
            "[Epoch 0/1] [Batch 37/938] [D loss: 0.480218] [G loss: 0.692934]\n",
            "[Epoch 0/1] [Batch 38/938] [D loss: 0.499896] [G loss: 0.530018]\n",
            "[Epoch 0/1] [Batch 39/938] [D loss: 0.507215] [G loss: 0.727039]\n",
            "[Epoch 0/1] [Batch 40/938] [D loss: 0.534948] [G loss: 0.496800]\n",
            "[Epoch 0/1] [Batch 41/938] [D loss: 0.518362] [G loss: 0.648404]\n",
            "[Epoch 0/1] [Batch 42/938] [D loss: 0.534232] [G loss: 0.591234]\n",
            "[Epoch 0/1] [Batch 43/938] [D loss: 0.539663] [G loss: 0.567024]\n",
            "[Epoch 0/1] [Batch 44/938] [D loss: 0.526716] [G loss: 0.641599]\n",
            "[Epoch 0/1] [Batch 45/938] [D loss: 0.512222] [G loss: 0.605674]\n",
            "[Epoch 0/1] [Batch 46/938] [D loss: 0.516063] [G loss: 0.718520]\n",
            "[Epoch 0/1] [Batch 47/938] [D loss: 0.516472] [G loss: 0.583201]\n",
            "[Epoch 0/1] [Batch 48/938] [D loss: 0.479725] [G loss: 0.750405]\n",
            "[Epoch 0/1] [Batch 49/938] [D loss: 0.481953] [G loss: 0.711434]\n",
            "[Epoch 0/1] [Batch 50/938] [D loss: 0.483397] [G loss: 0.791182]\n",
            "[Epoch 0/1] [Batch 51/938] [D loss: 0.505239] [G loss: 0.627241]\n",
            "[Epoch 0/1] [Batch 52/938] [D loss: 0.462591] [G loss: 0.907586]\n",
            "[Epoch 0/1] [Batch 53/938] [D loss: 0.485105] [G loss: 0.650826]\n",
            "[Epoch 0/1] [Batch 54/938] [D loss: 0.491892] [G loss: 0.909005]\n",
            "[Epoch 0/1] [Batch 55/938] [D loss: 0.559449] [G loss: 0.536679]\n",
            "[Epoch 0/1] [Batch 56/938] [D loss: 0.548709] [G loss: 0.972750]\n",
            "[Epoch 0/1] [Batch 57/938] [D loss: 0.616391] [G loss: 0.452906]\n",
            "[Epoch 0/1] [Batch 58/938] [D loss: 0.546449] [G loss: 0.826114]\n",
            "[Epoch 0/1] [Batch 59/938] [D loss: 0.536221] [G loss: 0.663772]\n",
            "[Epoch 0/1] [Batch 60/938] [D loss: 0.540488] [G loss: 0.636843]\n",
            "[Epoch 0/1] [Batch 61/938] [D loss: 0.567822] [G loss: 0.854004]\n",
            "[Epoch 0/1] [Batch 62/938] [D loss: 0.615546] [G loss: 0.425603]\n",
            "[Epoch 0/1] [Batch 63/938] [D loss: 0.491321] [G loss: 0.936228]\n",
            "[Epoch 0/1] [Batch 64/938] [D loss: 0.478315] [G loss: 0.701971]\n",
            "[Epoch 0/1] [Batch 65/938] [D loss: 0.472865] [G loss: 0.689099]\n",
            "[Epoch 0/1] [Batch 66/938] [D loss: 0.517975] [G loss: 0.888739]\n",
            "[Epoch 0/1] [Batch 67/938] [D loss: 0.542343] [G loss: 0.533932]\n",
            "[Epoch 0/1] [Batch 68/938] [D loss: 0.459383] [G loss: 0.835317]\n",
            "[Epoch 0/1] [Batch 69/938] [D loss: 0.498336] [G loss: 0.775997]\n",
            "[Epoch 0/1] [Batch 70/938] [D loss: 0.483249] [G loss: 0.601981]\n",
            "[Epoch 0/1] [Batch 71/938] [D loss: 0.421669] [G loss: 1.093275]\n",
            "[Epoch 0/1] [Batch 72/938] [D loss: 0.449183] [G loss: 0.667805]\n",
            "[Epoch 0/1] [Batch 73/938] [D loss: 0.448554] [G loss: 0.921709]\n",
            "[Epoch 0/1] [Batch 74/938] [D loss: 0.446570] [G loss: 0.721878]\n",
            "[Epoch 0/1] [Batch 75/938] [D loss: 0.443394] [G loss: 0.950088]\n",
            "[Epoch 0/1] [Batch 76/938] [D loss: 0.464932] [G loss: 0.703541]\n",
            "[Epoch 0/1] [Batch 77/938] [D loss: 0.420908] [G loss: 0.899538]\n",
            "[Epoch 0/1] [Batch 78/938] [D loss: 0.452544] [G loss: 0.817672]\n",
            "[Epoch 0/1] [Batch 79/938] [D loss: 0.460165] [G loss: 0.827834]\n",
            "[Epoch 0/1] [Batch 80/938] [D loss: 0.498344] [G loss: 0.853112]\n",
            "[Epoch 0/1] [Batch 81/938] [D loss: 0.509036] [G loss: 0.589809]\n",
            "[Epoch 0/1] [Batch 82/938] [D loss: 0.434748] [G loss: 1.094930]\n",
            "[Epoch 0/1] [Batch 83/938] [D loss: 0.487359] [G loss: 0.628444]\n",
            "[Epoch 0/1] [Batch 84/938] [D loss: 0.472369] [G loss: 0.962379]\n",
            "[Epoch 0/1] [Batch 85/938] [D loss: 0.508386] [G loss: 0.609089]\n",
            "[Epoch 0/1] [Batch 86/938] [D loss: 0.486412] [G loss: 0.925709]\n",
            "[Epoch 0/1] [Batch 87/938] [D loss: 0.542697] [G loss: 0.642037]\n",
            "[Epoch 0/1] [Batch 88/938] [D loss: 0.495568] [G loss: 0.748022]\n",
            "[Epoch 0/1] [Batch 89/938] [D loss: 0.432814] [G loss: 0.834150]\n",
            "[Epoch 0/1] [Batch 90/938] [D loss: 0.451862] [G loss: 0.799972]\n",
            "[Epoch 0/1] [Batch 91/938] [D loss: 0.463485] [G loss: 0.742112]\n",
            "[Epoch 0/1] [Batch 92/938] [D loss: 0.449633] [G loss: 0.880947]\n",
            "[Epoch 0/1] [Batch 93/938] [D loss: 0.446068] [G loss: 0.795668]\n",
            "[Epoch 0/1] [Batch 94/938] [D loss: 0.475332] [G loss: 0.905409]\n",
            "[Epoch 0/1] [Batch 95/938] [D loss: 0.456093] [G loss: 0.760422]\n",
            "[Epoch 0/1] [Batch 96/938] [D loss: 0.401025] [G loss: 0.843771]\n",
            "[Epoch 0/1] [Batch 97/938] [D loss: 0.370517] [G loss: 1.078701]\n",
            "[Epoch 0/1] [Batch 98/938] [D loss: 0.383917] [G loss: 0.911551]\n",
            "[Epoch 0/1] [Batch 99/938] [D loss: 0.381172] [G loss: 0.926182]\n",
            "[Epoch 0/1] [Batch 100/938] [D loss: 0.400250] [G loss: 1.021636]\n",
            "[Epoch 0/1] [Batch 101/938] [D loss: 0.389190] [G loss: 0.889033]\n",
            "[Epoch 0/1] [Batch 102/938] [D loss: 0.368666] [G loss: 1.054775]\n",
            "[Epoch 0/1] [Batch 103/938] [D loss: 0.379882] [G loss: 0.924769]\n",
            "[Epoch 0/1] [Batch 104/938] [D loss: 0.370406] [G loss: 1.022319]\n",
            "[Epoch 0/1] [Batch 105/938] [D loss: 0.396038] [G loss: 0.959478]\n",
            "[Epoch 0/1] [Batch 106/938] [D loss: 0.423055] [G loss: 0.926681]\n",
            "[Epoch 0/1] [Batch 107/938] [D loss: 0.452019] [G loss: 0.888518]\n",
            "[Epoch 0/1] [Batch 108/938] [D loss: 0.439518] [G loss: 0.884597]\n",
            "[Epoch 0/1] [Batch 109/938] [D loss: 0.465175] [G loss: 0.856868]\n",
            "[Epoch 0/1] [Batch 110/938] [D loss: 0.478813] [G loss: 0.861144]\n",
            "[Epoch 0/1] [Batch 111/938] [D loss: 0.472701] [G loss: 0.803671]\n",
            "[Epoch 0/1] [Batch 112/938] [D loss: 0.472602] [G loss: 0.829235]\n",
            "[Epoch 0/1] [Batch 113/938] [D loss: 0.460750] [G loss: 0.895470]\n",
            "[Epoch 0/1] [Batch 114/938] [D loss: 0.479051] [G loss: 0.806027]\n",
            "[Epoch 0/1] [Batch 115/938] [D loss: 0.470691] [G loss: 0.835552]\n",
            "[Epoch 0/1] [Batch 116/938] [D loss: 0.473169] [G loss: 0.850695]\n",
            "[Epoch 0/1] [Batch 117/938] [D loss: 0.465757] [G loss: 0.775514]\n",
            "[Epoch 0/1] [Batch 118/938] [D loss: 0.448246] [G loss: 0.858984]\n",
            "[Epoch 0/1] [Batch 119/938] [D loss: 0.426955] [G loss: 0.928673]\n",
            "[Epoch 0/1] [Batch 120/938] [D loss: 0.409760] [G loss: 0.923962]\n",
            "[Epoch 0/1] [Batch 121/938] [D loss: 0.436522] [G loss: 0.860770]\n",
            "[Epoch 0/1] [Batch 122/938] [D loss: 0.469400] [G loss: 0.814055]\n",
            "[Epoch 0/1] [Batch 123/938] [D loss: 0.487305] [G loss: 0.799795]\n",
            "[Epoch 0/1] [Batch 124/938] [D loss: 0.571466] [G loss: 0.618775]\n",
            "[Epoch 0/1] [Batch 125/938] [D loss: 0.600321] [G loss: 0.698331]\n",
            "[Epoch 0/1] [Batch 126/938] [D loss: 0.613881] [G loss: 0.654458]\n",
            "[Epoch 0/1] [Batch 127/938] [D loss: 0.681013] [G loss: 0.459428]\n",
            "[Epoch 0/1] [Batch 128/938] [D loss: 0.678837] [G loss: 0.788349]\n",
            "[Epoch 0/1] [Batch 129/938] [D loss: 0.701035] [G loss: 0.412302]\n",
            "[Epoch 0/1] [Batch 130/938] [D loss: 0.654246] [G loss: 0.683563]\n",
            "[Epoch 0/1] [Batch 131/938] [D loss: 0.612841] [G loss: 0.616326]\n",
            "[Epoch 0/1] [Batch 132/938] [D loss: 0.623130] [G loss: 0.637313]\n",
            "[Epoch 0/1] [Batch 133/938] [D loss: 0.621287] [G loss: 0.616416]\n",
            "[Epoch 0/1] [Batch 134/938] [D loss: 0.607925] [G loss: 0.669985]\n",
            "[Epoch 0/1] [Batch 135/938] [D loss: 0.608367] [G loss: 0.745343]\n",
            "[Epoch 0/1] [Batch 136/938] [D loss: 0.631320] [G loss: 0.611638]\n",
            "[Epoch 0/1] [Batch 137/938] [D loss: 0.619138] [G loss: 0.674716]\n",
            "[Epoch 0/1] [Batch 138/938] [D loss: 0.621203] [G loss: 0.729581]\n",
            "[Epoch 0/1] [Batch 139/938] [D loss: 0.665669] [G loss: 0.552609]\n",
            "[Epoch 0/1] [Batch 140/938] [D loss: 0.656434] [G loss: 0.844519]\n",
            "[Epoch 0/1] [Batch 141/938] [D loss: 0.657213] [G loss: 0.552214]\n",
            "[Epoch 0/1] [Batch 142/938] [D loss: 0.581873] [G loss: 0.761474]\n",
            "[Epoch 0/1] [Batch 143/938] [D loss: 0.561282] [G loss: 0.760935]\n",
            "[Epoch 0/1] [Batch 144/938] [D loss: 0.618850] [G loss: 0.701150]\n",
            "[Epoch 0/1] [Batch 145/938] [D loss: 0.637130] [G loss: 0.618714]\n",
            "[Epoch 0/1] [Batch 146/938] [D loss: 0.688189] [G loss: 0.627658]\n",
            "[Epoch 0/1] [Batch 147/938] [D loss: 0.707349] [G loss: 0.561234]\n",
            "[Epoch 0/1] [Batch 148/938] [D loss: 0.685684] [G loss: 0.665700]\n",
            "[Epoch 0/1] [Batch 149/938] [D loss: 0.725896] [G loss: 0.447895]\n",
            "[Epoch 0/1] [Batch 150/938] [D loss: 0.733734] [G loss: 0.809603]\n",
            "[Epoch 0/1] [Batch 151/938] [D loss: 0.702979] [G loss: 0.446817]\n",
            "[Epoch 0/1] [Batch 152/938] [D loss: 0.677808] [G loss: 0.614917]\n",
            "[Epoch 0/1] [Batch 153/938] [D loss: 0.645232] [G loss: 0.784559]\n",
            "[Epoch 0/1] [Batch 154/938] [D loss: 0.658608] [G loss: 0.549716]\n",
            "[Epoch 0/1] [Batch 155/938] [D loss: 0.598055] [G loss: 0.673987]\n",
            "[Epoch 0/1] [Batch 156/938] [D loss: 0.557920] [G loss: 0.771507]\n",
            "[Epoch 0/1] [Batch 157/938] [D loss: 0.583929] [G loss: 0.663147]\n",
            "[Epoch 0/1] [Batch 158/938] [D loss: 0.569984] [G loss: 0.726550]\n",
            "[Epoch 0/1] [Batch 159/938] [D loss: 0.600803] [G loss: 0.739346]\n",
            "[Epoch 0/1] [Batch 160/938] [D loss: 0.617135] [G loss: 0.689447]\n",
            "[Epoch 0/1] [Batch 161/938] [D loss: 0.647624] [G loss: 0.583774]\n",
            "[Epoch 0/1] [Batch 162/938] [D loss: 0.658807] [G loss: 0.716512]\n",
            "[Epoch 0/1] [Batch 163/938] [D loss: 0.627887] [G loss: 0.584427]\n",
            "[Epoch 0/1] [Batch 164/938] [D loss: 0.577362] [G loss: 0.947180]\n",
            "[Epoch 0/1] [Batch 165/938] [D loss: 0.574749] [G loss: 0.575770]\n",
            "[Epoch 0/1] [Batch 166/938] [D loss: 0.528096] [G loss: 0.951972]\n",
            "[Epoch 0/1] [Batch 167/938] [D loss: 0.536921] [G loss: 0.695483]\n",
            "[Epoch 0/1] [Batch 168/938] [D loss: 0.543085] [G loss: 0.854266]\n",
            "[Epoch 0/1] [Batch 169/938] [D loss: 0.582125] [G loss: 0.665255]\n",
            "[Epoch 0/1] [Batch 170/938] [D loss: 0.602910] [G loss: 0.900645]\n",
            "[Epoch 0/1] [Batch 171/938] [D loss: 0.708154] [G loss: 0.412621]\n",
            "[Epoch 0/1] [Batch 172/938] [D loss: 0.752321] [G loss: 1.221638]\n",
            "[Epoch 0/1] [Batch 173/938] [D loss: 0.830470] [G loss: 0.277344]\n",
            "[Epoch 0/1] [Batch 174/938] [D loss: 0.658219] [G loss: 0.831030]\n",
            "[Epoch 0/1] [Batch 175/938] [D loss: 0.683744] [G loss: 0.702994]\n",
            "[Epoch 0/1] [Batch 176/938] [D loss: 0.695688] [G loss: 0.477045]\n",
            "[Epoch 0/1] [Batch 177/938] [D loss: 0.638597] [G loss: 0.746497]\n",
            "[Epoch 0/1] [Batch 178/938] [D loss: 0.642138] [G loss: 0.706098]\n",
            "[Epoch 0/1] [Batch 179/938] [D loss: 0.649412] [G loss: 0.586408]\n",
            "[Epoch 0/1] [Batch 180/938] [D loss: 0.655499] [G loss: 0.761870]\n",
            "[Epoch 0/1] [Batch 181/938] [D loss: 0.623218] [G loss: 0.633343]\n",
            "[Epoch 0/1] [Batch 182/938] [D loss: 0.600016] [G loss: 0.745435]\n",
            "[Epoch 0/1] [Batch 183/938] [D loss: 0.553331] [G loss: 0.754431]\n",
            "[Epoch 0/1] [Batch 184/938] [D loss: 0.574341] [G loss: 0.811967]\n",
            "[Epoch 0/1] [Batch 185/938] [D loss: 0.576476] [G loss: 0.681962]\n",
            "[Epoch 0/1] [Batch 186/938] [D loss: 0.573139] [G loss: 0.799174]\n",
            "[Epoch 0/1] [Batch 187/938] [D loss: 0.596006] [G loss: 0.767415]\n",
            "[Epoch 0/1] [Batch 188/938] [D loss: 0.631513] [G loss: 0.624950]\n",
            "[Epoch 0/1] [Batch 189/938] [D loss: 0.596607] [G loss: 0.720002]\n",
            "[Epoch 0/1] [Batch 190/938] [D loss: 0.641836] [G loss: 0.764992]\n",
            "[Epoch 0/1] [Batch 191/938] [D loss: 0.658041] [G loss: 0.565695]\n",
            "[Epoch 0/1] [Batch 192/938] [D loss: 0.599704] [G loss: 0.812054]\n",
            "[Epoch 0/1] [Batch 193/938] [D loss: 0.619096] [G loss: 0.760185]\n",
            "[Epoch 0/1] [Batch 194/938] [D loss: 0.576604] [G loss: 0.619857]\n",
            "[Epoch 0/1] [Batch 195/938] [D loss: 0.515001] [G loss: 1.038684]\n",
            "[Epoch 0/1] [Batch 196/938] [D loss: 0.480008] [G loss: 0.792625]\n",
            "[Epoch 0/1] [Batch 197/938] [D loss: 0.497423] [G loss: 0.956119]\n",
            "[Epoch 0/1] [Batch 198/938] [D loss: 0.524744] [G loss: 0.769714]\n",
            "[Epoch 0/1] [Batch 199/938] [D loss: 0.532642] [G loss: 0.847197]\n",
            "[Epoch 0/1] [Batch 200/938] [D loss: 0.584492] [G loss: 0.740284]\n",
            "[Epoch 0/1] [Batch 201/938] [D loss: 0.596430] [G loss: 0.695504]\n",
            "[Epoch 0/1] [Batch 202/938] [D loss: 0.602887] [G loss: 0.845873]\n",
            "[Epoch 0/1] [Batch 203/938] [D loss: 0.609311] [G loss: 0.592386]\n",
            "[Epoch 0/1] [Batch 204/938] [D loss: 0.577662] [G loss: 0.828432]\n",
            "[Epoch 0/1] [Batch 205/938] [D loss: 0.586892] [G loss: 0.755678]\n",
            "[Epoch 0/1] [Batch 206/938] [D loss: 0.573609] [G loss: 0.691634]\n",
            "[Epoch 0/1] [Batch 207/938] [D loss: 0.566303] [G loss: 0.884757]\n",
            "[Epoch 0/1] [Batch 208/938] [D loss: 0.540422] [G loss: 0.688646]\n",
            "[Epoch 0/1] [Batch 209/938] [D loss: 0.506099] [G loss: 1.095527]\n",
            "[Epoch 0/1] [Batch 210/938] [D loss: 0.507711] [G loss: 0.663272]\n",
            "[Epoch 0/1] [Batch 211/938] [D loss: 0.478966] [G loss: 1.220584]\n",
            "[Epoch 0/1] [Batch 212/938] [D loss: 0.538777] [G loss: 0.625762]\n",
            "[Epoch 0/1] [Batch 213/938] [D loss: 0.576780] [G loss: 0.998171]\n",
            "[Epoch 0/1] [Batch 214/938] [D loss: 0.620257] [G loss: 0.493425]\n",
            "[Epoch 0/1] [Batch 215/938] [D loss: 0.668476] [G loss: 1.283447]\n",
            "[Epoch 0/1] [Batch 216/938] [D loss: 0.778424] [G loss: 0.296959]\n",
            "[Epoch 0/1] [Batch 217/938] [D loss: 0.693652] [G loss: 1.215616]\n",
            "[Epoch 0/1] [Batch 218/938] [D loss: 0.666674] [G loss: 0.510162]\n",
            "[Epoch 0/1] [Batch 219/938] [D loss: 0.596369] [G loss: 0.786627]\n",
            "[Epoch 0/1] [Batch 220/938] [D loss: 0.568362] [G loss: 0.804882]\n",
            "[Epoch 0/1] [Batch 221/938] [D loss: 0.519862] [G loss: 0.773050]\n",
            "[Epoch 0/1] [Batch 222/938] [D loss: 0.482803] [G loss: 1.029653]\n",
            "[Epoch 0/1] [Batch 223/938] [D loss: 0.482339] [G loss: 0.784258]\n",
            "[Epoch 0/1] [Batch 224/938] [D loss: 0.465125] [G loss: 1.095695]\n",
            "[Epoch 0/1] [Batch 225/938] [D loss: 0.473203] [G loss: 0.780151]\n",
            "[Epoch 0/1] [Batch 226/938] [D loss: 0.477197] [G loss: 1.156175]\n",
            "[Epoch 0/1] [Batch 227/938] [D loss: 0.524609] [G loss: 0.688167]\n",
            "[Epoch 0/1] [Batch 228/938] [D loss: 0.529541] [G loss: 1.133202]\n",
            "[Epoch 0/1] [Batch 229/938] [D loss: 0.610884] [G loss: 0.531413]\n",
            "[Epoch 0/1] [Batch 230/938] [D loss: 0.615939] [G loss: 1.207820]\n",
            "[Epoch 0/1] [Batch 231/938] [D loss: 0.702364] [G loss: 0.386198]\n",
            "[Epoch 0/1] [Batch 232/938] [D loss: 0.622591] [G loss: 1.199169]\n",
            "[Epoch 0/1] [Batch 233/938] [D loss: 0.578115] [G loss: 0.598449]\n",
            "[Epoch 0/1] [Batch 234/938] [D loss: 0.500109] [G loss: 0.863517]\n",
            "[Epoch 0/1] [Batch 235/938] [D loss: 0.497561] [G loss: 1.137186]\n",
            "[Epoch 0/1] [Batch 236/938] [D loss: 0.490532] [G loss: 0.655286]\n",
            "[Epoch 0/1] [Batch 237/938] [D loss: 0.454074] [G loss: 1.364045]\n",
            "[Epoch 0/1] [Batch 238/938] [D loss: 0.414225] [G loss: 0.788124]\n",
            "[Epoch 0/1] [Batch 239/938] [D loss: 0.453137] [G loss: 1.277691]\n",
            "[Epoch 0/1] [Batch 240/938] [D loss: 0.517195] [G loss: 0.648001]\n",
            "[Epoch 0/1] [Batch 241/938] [D loss: 0.469969] [G loss: 1.234909]\n",
            "[Epoch 0/1] [Batch 242/938] [D loss: 0.542528] [G loss: 0.653208]\n",
            "[Epoch 0/1] [Batch 243/938] [D loss: 0.529454] [G loss: 1.135750]\n",
            "[Epoch 0/1] [Batch 244/938] [D loss: 0.608594] [G loss: 0.534655]\n",
            "[Epoch 0/1] [Batch 245/938] [D loss: 0.633430] [G loss: 1.165887]\n",
            "[Epoch 0/1] [Batch 246/938] [D loss: 0.736622] [G loss: 0.370205]\n",
            "[Epoch 0/1] [Batch 247/938] [D loss: 0.700752] [G loss: 1.122900]\n",
            "[Epoch 0/1] [Batch 248/938] [D loss: 0.678073] [G loss: 0.427242]\n",
            "[Epoch 0/1] [Batch 249/938] [D loss: 0.630084] [G loss: 1.091975]\n",
            "[Epoch 0/1] [Batch 250/938] [D loss: 0.586190] [G loss: 0.532395]\n",
            "[Epoch 0/1] [Batch 251/938] [D loss: 0.527730] [G loss: 1.239089]\n",
            "[Epoch 0/1] [Batch 252/938] [D loss: 0.510117] [G loss: 0.707426]\n",
            "[Epoch 0/1] [Batch 253/938] [D loss: 0.458589] [G loss: 1.186349]\n",
            "[Epoch 0/1] [Batch 254/938] [D loss: 0.425742] [G loss: 0.955772]\n",
            "[Epoch 0/1] [Batch 255/938] [D loss: 0.383381] [G loss: 1.163850]\n",
            "[Epoch 0/1] [Batch 256/938] [D loss: 0.409607] [G loss: 1.139204]\n",
            "[Epoch 0/1] [Batch 257/938] [D loss: 0.446218] [G loss: 0.907297]\n",
            "[Epoch 0/1] [Batch 258/938] [D loss: 0.477647] [G loss: 1.223700]\n",
            "[Epoch 0/1] [Batch 259/938] [D loss: 0.549202] [G loss: 0.614097]\n",
            "[Epoch 0/1] [Batch 260/938] [D loss: 0.564438] [G loss: 1.614912]\n",
            "[Epoch 0/1] [Batch 261/938] [D loss: 0.683640] [G loss: 0.409871]\n",
            "[Epoch 0/1] [Batch 262/938] [D loss: 0.534505] [G loss: 1.586036]\n",
            "[Epoch 0/1] [Batch 263/938] [D loss: 0.525306] [G loss: 0.637847]\n",
            "[Epoch 0/1] [Batch 264/938] [D loss: 0.477563] [G loss: 1.128400]\n",
            "[Epoch 0/1] [Batch 265/938] [D loss: 0.481621] [G loss: 0.847420]\n",
            "[Epoch 0/1] [Batch 266/938] [D loss: 0.498824] [G loss: 0.946734]\n",
            "[Epoch 0/1] [Batch 267/938] [D loss: 0.523915] [G loss: 0.766275]\n",
            "[Epoch 0/1] [Batch 268/938] [D loss: 0.533628] [G loss: 1.052702]\n",
            "[Epoch 0/1] [Batch 269/938] [D loss: 0.531662] [G loss: 0.555446]\n",
            "[Epoch 0/1] [Batch 270/938] [D loss: 0.511327] [G loss: 1.587111]\n",
            "[Epoch 0/1] [Batch 271/938] [D loss: 0.519455] [G loss: 0.546973]\n",
            "[Epoch 0/1] [Batch 272/938] [D loss: 0.414205] [G loss: 1.379820]\n",
            "[Epoch 0/1] [Batch 273/938] [D loss: 0.416018] [G loss: 0.843838]\n",
            "[Epoch 0/1] [Batch 274/938] [D loss: 0.403610] [G loss: 1.006142]\n",
            "[Epoch 0/1] [Batch 275/938] [D loss: 0.445097] [G loss: 1.040458]\n",
            "[Epoch 0/1] [Batch 276/938] [D loss: 0.483859] [G loss: 0.741047]\n",
            "[Epoch 0/1] [Batch 277/938] [D loss: 0.469230] [G loss: 1.087381]\n",
            "[Epoch 0/1] [Batch 278/938] [D loss: 0.495538] [G loss: 0.652357]\n",
            "[Epoch 0/1] [Batch 279/938] [D loss: 0.478020] [G loss: 1.205448]\n",
            "[Epoch 0/1] [Batch 280/938] [D loss: 0.529683] [G loss: 0.566745]\n",
            "[Epoch 0/1] [Batch 281/938] [D loss: 0.504219] [G loss: 1.335762]\n",
            "[Epoch 0/1] [Batch 282/938] [D loss: 0.570489] [G loss: 0.489497]\n",
            "[Epoch 0/1] [Batch 283/938] [D loss: 0.502966] [G loss: 1.296059]\n",
            "[Epoch 0/1] [Batch 284/938] [D loss: 0.497180] [G loss: 0.617111]\n",
            "[Epoch 0/1] [Batch 285/938] [D loss: 0.435448] [G loss: 1.090223]\n",
            "[Epoch 0/1] [Batch 286/938] [D loss: 0.429187] [G loss: 0.874740]\n",
            "[Epoch 0/1] [Batch 287/938] [D loss: 0.446749] [G loss: 0.879238]\n",
            "[Epoch 0/1] [Batch 288/938] [D loss: 0.436841] [G loss: 0.924325]\n",
            "[Epoch 0/1] [Batch 289/938] [D loss: 0.454620] [G loss: 0.785408]\n",
            "[Epoch 0/1] [Batch 290/938] [D loss: 0.456929] [G loss: 1.027359]\n",
            "[Epoch 0/1] [Batch 291/938] [D loss: 0.509257] [G loss: 0.662534]\n",
            "[Epoch 0/1] [Batch 292/938] [D loss: 0.419854] [G loss: 1.064087]\n",
            "[Epoch 0/1] [Batch 293/938] [D loss: 0.455129] [G loss: 0.770288]\n",
            "[Epoch 0/1] [Batch 294/938] [D loss: 0.434727] [G loss: 1.006003]\n",
            "[Epoch 0/1] [Batch 295/938] [D loss: 0.439804] [G loss: 0.738117]\n",
            "[Epoch 0/1] [Batch 296/938] [D loss: 0.414016] [G loss: 1.143080]\n",
            "[Epoch 0/1] [Batch 297/938] [D loss: 0.451525] [G loss: 0.675241]\n",
            "[Epoch 0/1] [Batch 298/938] [D loss: 0.460026] [G loss: 1.328956]\n",
            "[Epoch 0/1] [Batch 299/938] [D loss: 0.511076] [G loss: 0.600164]\n",
            "[Epoch 0/1] [Batch 300/938] [D loss: 0.398937] [G loss: 1.312631]\n",
            "[Epoch 0/1] [Batch 301/938] [D loss: 0.397277] [G loss: 0.815880]\n",
            "[Epoch 0/1] [Batch 302/938] [D loss: 0.417196] [G loss: 0.907108]\n",
            "[Epoch 0/1] [Batch 303/938] [D loss: 0.469152] [G loss: 0.890386]\n",
            "[Epoch 0/1] [Batch 304/938] [D loss: 0.507622] [G loss: 0.771531]\n",
            "[Epoch 0/1] [Batch 305/938] [D loss: 0.526593] [G loss: 0.726522]\n",
            "[Epoch 0/1] [Batch 306/938] [D loss: 0.486263] [G loss: 0.754656]\n",
            "[Epoch 0/1] [Batch 307/938] [D loss: 0.468123] [G loss: 0.949949]\n",
            "[Epoch 0/1] [Batch 308/938] [D loss: 0.543163] [G loss: 0.560896]\n",
            "[Epoch 0/1] [Batch 309/938] [D loss: 0.546327] [G loss: 1.215462]\n",
            "[Epoch 0/1] [Batch 310/938] [D loss: 0.606451] [G loss: 0.441602]\n",
            "[Epoch 0/1] [Batch 311/938] [D loss: 0.491216] [G loss: 1.232743]\n",
            "[Epoch 0/1] [Batch 312/938] [D loss: 0.460685] [G loss: 0.667238]\n",
            "[Epoch 0/1] [Batch 313/938] [D loss: 0.419464] [G loss: 1.005605]\n",
            "[Epoch 0/1] [Batch 314/938] [D loss: 0.415173] [G loss: 0.899294]\n",
            "[Epoch 0/1] [Batch 315/938] [D loss: 0.427115] [G loss: 0.972328]\n",
            "[Epoch 0/1] [Batch 316/938] [D loss: 0.408099] [G loss: 0.911782]\n",
            "[Epoch 0/1] [Batch 317/938] [D loss: 0.412954] [G loss: 1.007930]\n",
            "[Epoch 0/1] [Batch 318/938] [D loss: 0.431831] [G loss: 0.871863]\n",
            "[Epoch 0/1] [Batch 319/938] [D loss: 0.433954] [G loss: 0.976331]\n",
            "[Epoch 0/1] [Batch 320/938] [D loss: 0.438226] [G loss: 0.871797]\n",
            "[Epoch 0/1] [Batch 321/938] [D loss: 0.435798] [G loss: 1.097028]\n",
            "[Epoch 0/1] [Batch 322/938] [D loss: 0.489618] [G loss: 0.691564]\n",
            "[Epoch 0/1] [Batch 323/938] [D loss: 0.529426] [G loss: 1.426394]\n",
            "[Epoch 0/1] [Batch 324/938] [D loss: 0.688228] [G loss: 0.357869]\n",
            "[Epoch 0/1] [Batch 325/938] [D loss: 0.572082] [G loss: 1.681462]\n",
            "[Epoch 0/1] [Batch 326/938] [D loss: 0.537855] [G loss: 0.623074]\n",
            "[Epoch 0/1] [Batch 327/938] [D loss: 0.471386] [G loss: 0.991901]\n",
            "[Epoch 0/1] [Batch 328/938] [D loss: 0.481839] [G loss: 1.078785]\n",
            "[Epoch 0/1] [Batch 329/938] [D loss: 0.507461] [G loss: 0.789677]\n",
            "[Epoch 0/1] [Batch 330/938] [D loss: 0.466920] [G loss: 0.970836]\n",
            "[Epoch 0/1] [Batch 331/938] [D loss: 0.494203] [G loss: 1.078245]\n",
            "[Epoch 0/1] [Batch 332/938] [D loss: 0.512948] [G loss: 0.687727]\n",
            "[Epoch 0/1] [Batch 333/938] [D loss: 0.470222] [G loss: 1.154622]\n",
            "[Epoch 0/1] [Batch 334/938] [D loss: 0.468814] [G loss: 0.892914]\n",
            "[Epoch 0/1] [Batch 335/938] [D loss: 0.458291] [G loss: 0.912361]\n",
            "[Epoch 0/1] [Batch 336/938] [D loss: 0.473489] [G loss: 1.082357]\n",
            "[Epoch 0/1] [Batch 337/938] [D loss: 0.471664] [G loss: 0.767319]\n",
            "[Epoch 0/1] [Batch 338/938] [D loss: 0.453788] [G loss: 1.248755]\n",
            "[Epoch 0/1] [Batch 339/938] [D loss: 0.463797] [G loss: 0.726024]\n",
            "[Epoch 0/1] [Batch 340/938] [D loss: 0.448415] [G loss: 1.197241]\n",
            "[Epoch 0/1] [Batch 341/938] [D loss: 0.456212] [G loss: 0.747805]\n",
            "[Epoch 0/1] [Batch 342/938] [D loss: 0.450108] [G loss: 1.345285]\n",
            "[Epoch 0/1] [Batch 343/938] [D loss: 0.479663] [G loss: 0.644801]\n",
            "[Epoch 0/1] [Batch 344/938] [D loss: 0.483310] [G loss: 1.525576]\n",
            "[Epoch 0/1] [Batch 345/938] [D loss: 0.582944] [G loss: 0.501403]\n",
            "[Epoch 0/1] [Batch 346/938] [D loss: 0.573933] [G loss: 1.588302]\n",
            "[Epoch 0/1] [Batch 347/938] [D loss: 0.647276] [G loss: 0.406441]\n",
            "[Epoch 0/1] [Batch 348/938] [D loss: 0.497556] [G loss: 1.515000]\n",
            "[Epoch 0/1] [Batch 349/938] [D loss: 0.473410] [G loss: 0.673749]\n",
            "[Epoch 0/1] [Batch 350/938] [D loss: 0.444984] [G loss: 1.089570]\n",
            "[Epoch 0/1] [Batch 351/938] [D loss: 0.417786] [G loss: 0.855698]\n",
            "[Epoch 0/1] [Batch 352/938] [D loss: 0.422585] [G loss: 1.213206]\n",
            "[Epoch 0/1] [Batch 353/938] [D loss: 0.466610] [G loss: 0.751164]\n",
            "[Epoch 0/1] [Batch 354/938] [D loss: 0.457610] [G loss: 1.301576]\n",
            "[Epoch 0/1] [Batch 355/938] [D loss: 0.479398] [G loss: 0.630241]\n",
            "[Epoch 0/1] [Batch 356/938] [D loss: 0.571246] [G loss: 1.558996]\n",
            "[Epoch 0/1] [Batch 357/938] [D loss: 0.653589] [G loss: 0.386429]\n",
            "[Epoch 0/1] [Batch 358/938] [D loss: 0.447084] [G loss: 1.316713]\n",
            "[Epoch 0/1] [Batch 359/938] [D loss: 0.453645] [G loss: 0.948610]\n",
            "[Epoch 0/1] [Batch 360/938] [D loss: 0.469716] [G loss: 0.739158]\n",
            "[Epoch 0/1] [Batch 361/938] [D loss: 0.482755] [G loss: 1.246847]\n",
            "[Epoch 0/1] [Batch 362/938] [D loss: 0.516853] [G loss: 0.633724]\n",
            "[Epoch 0/1] [Batch 363/938] [D loss: 0.457012] [G loss: 1.129921]\n",
            "[Epoch 0/1] [Batch 364/938] [D loss: 0.454855] [G loss: 0.820205]\n",
            "[Epoch 0/1] [Batch 365/938] [D loss: 0.461518] [G loss: 0.985877]\n",
            "[Epoch 0/1] [Batch 366/938] [D loss: 0.445431] [G loss: 0.821865]\n",
            "[Epoch 0/1] [Batch 367/938] [D loss: 0.481139] [G loss: 1.119447]\n",
            "[Epoch 0/1] [Batch 368/938] [D loss: 0.504235] [G loss: 0.629550]\n",
            "[Epoch 0/1] [Batch 369/938] [D loss: 0.491987] [G loss: 1.267982]\n",
            "[Epoch 0/1] [Batch 370/938] [D loss: 0.554256] [G loss: 0.519769]\n",
            "[Epoch 0/1] [Batch 371/938] [D loss: 0.573748] [G loss: 1.506264]\n",
            "[Epoch 0/1] [Batch 372/938] [D loss: 0.646892] [G loss: 0.396684]\n",
            "[Epoch 0/1] [Batch 373/938] [D loss: 0.496027] [G loss: 1.400323]\n",
            "[Epoch 0/1] [Batch 374/938] [D loss: 0.504428] [G loss: 0.670274]\n",
            "[Epoch 0/1] [Batch 375/938] [D loss: 0.489686] [G loss: 0.958860]\n",
            "[Epoch 0/1] [Batch 376/938] [D loss: 0.481185] [G loss: 0.872223]\n",
            "[Epoch 0/1] [Batch 377/938] [D loss: 0.463420] [G loss: 0.883159]\n",
            "[Epoch 0/1] [Batch 378/938] [D loss: 0.470349] [G loss: 1.024956]\n",
            "[Epoch 0/1] [Batch 379/938] [D loss: 0.461540] [G loss: 0.737507]\n",
            "[Epoch 0/1] [Batch 380/938] [D loss: 0.489850] [G loss: 1.309680]\n",
            "[Epoch 0/1] [Batch 381/938] [D loss: 0.555306] [G loss: 0.511901]\n",
            "[Epoch 0/1] [Batch 382/938] [D loss: 0.546088] [G loss: 1.532760]\n",
            "[Epoch 0/1] [Batch 383/938] [D loss: 0.579151] [G loss: 0.471953]\n",
            "[Epoch 0/1] [Batch 384/938] [D loss: 0.489689] [G loss: 1.319576]\n",
            "[Epoch 0/1] [Batch 385/938] [D loss: 0.536097] [G loss: 0.626725]\n",
            "[Epoch 0/1] [Batch 386/938] [D loss: 0.554460] [G loss: 1.015347]\n",
            "[Epoch 0/1] [Batch 387/938] [D loss: 0.601607] [G loss: 0.508535]\n",
            "[Epoch 0/1] [Batch 388/938] [D loss: 0.565241] [G loss: 1.235089]\n",
            "[Epoch 0/1] [Batch 389/938] [D loss: 0.651539] [G loss: 0.436204]\n",
            "[Epoch 0/1] [Batch 390/938] [D loss: 0.561064] [G loss: 1.222905]\n",
            "[Epoch 0/1] [Batch 391/938] [D loss: 0.549557] [G loss: 0.560894]\n",
            "[Epoch 0/1] [Batch 392/938] [D loss: 0.533154] [G loss: 1.128989]\n",
            "[Epoch 0/1] [Batch 393/938] [D loss: 0.506618] [G loss: 0.643033]\n",
            "[Epoch 0/1] [Batch 394/938] [D loss: 0.522724] [G loss: 1.245371]\n",
            "[Epoch 0/1] [Batch 395/938] [D loss: 0.536627] [G loss: 0.548332]\n",
            "[Epoch 0/1] [Batch 396/938] [D loss: 0.505327] [G loss: 1.434064]\n",
            "[Epoch 0/1] [Batch 397/938] [D loss: 0.501629] [G loss: 0.591953]\n",
            "[Epoch 0/1] [Batch 398/938] [D loss: 0.430007] [G loss: 1.402093]\n",
            "[Epoch 0/1] [Batch 399/938] [D loss: 0.421135] [G loss: 0.805470]\n",
            "[Epoch 0/1] [Batch 400/938] [D loss: 0.428115] [G loss: 1.111780]\n",
            "[Epoch 0/1] [Batch 401/938] [D loss: 0.440701] [G loss: 0.846410]\n",
            "[Epoch 0/1] [Batch 402/938] [D loss: 0.479276] [G loss: 1.087857]\n",
            "[Epoch 0/1] [Batch 403/938] [D loss: 0.519110] [G loss: 0.647077]\n",
            "[Epoch 0/1] [Batch 404/938] [D loss: 0.526165] [G loss: 1.266918]\n",
            "[Epoch 0/1] [Batch 405/938] [D loss: 0.659313] [G loss: 0.402657]\n",
            "[Epoch 0/1] [Batch 406/938] [D loss: 0.581802] [G loss: 1.470424]\n",
            "[Epoch 0/1] [Batch 407/938] [D loss: 0.608751] [G loss: 0.445938]\n",
            "[Epoch 0/1] [Batch 408/938] [D loss: 0.515633] [G loss: 1.160221]\n",
            "[Epoch 0/1] [Batch 409/938] [D loss: 0.498503] [G loss: 0.681752]\n",
            "[Epoch 0/1] [Batch 410/938] [D loss: 0.474834] [G loss: 0.916634]\n",
            "[Epoch 0/1] [Batch 411/938] [D loss: 0.500603] [G loss: 0.826962]\n",
            "[Epoch 0/1] [Batch 412/938] [D loss: 0.516510] [G loss: 0.730490]\n",
            "[Epoch 0/1] [Batch 413/938] [D loss: 0.511193] [G loss: 0.862546]\n",
            "[Epoch 0/1] [Batch 414/938] [D loss: 0.543951] [G loss: 0.730683]\n",
            "[Epoch 0/1] [Batch 415/938] [D loss: 0.526425] [G loss: 0.796368]\n",
            "[Epoch 0/1] [Batch 416/938] [D loss: 0.526327] [G loss: 0.829799]\n",
            "[Epoch 0/1] [Batch 417/938] [D loss: 0.483779] [G loss: 0.764157]\n",
            "[Epoch 0/1] [Batch 418/938] [D loss: 0.502343] [G loss: 1.036647]\n",
            "[Epoch 0/1] [Batch 419/938] [D loss: 0.555298] [G loss: 0.547700]\n",
            "[Epoch 0/1] [Batch 420/938] [D loss: 0.487686] [G loss: 1.402368]\n",
            "[Epoch 0/1] [Batch 421/938] [D loss: 0.471548] [G loss: 0.634325]\n",
            "[Epoch 0/1] [Batch 422/938] [D loss: 0.377565] [G loss: 0.979816]\n",
            "[Epoch 0/1] [Batch 423/938] [D loss: 0.388998] [G loss: 1.316356]\n",
            "[Epoch 0/1] [Batch 424/938] [D loss: 0.434052] [G loss: 0.680479]\n",
            "[Epoch 0/1] [Batch 425/938] [D loss: 0.400680] [G loss: 1.263623]\n",
            "[Epoch 0/1] [Batch 426/938] [D loss: 0.444237] [G loss: 0.774689]\n",
            "[Epoch 0/1] [Batch 427/938] [D loss: 0.421124] [G loss: 0.986640]\n",
            "[Epoch 0/1] [Batch 428/938] [D loss: 0.453201] [G loss: 0.874529]\n",
            "[Epoch 0/1] [Batch 429/938] [D loss: 0.458477] [G loss: 0.875786]\n",
            "[Epoch 0/1] [Batch 430/938] [D loss: 0.478481] [G loss: 0.901654]\n",
            "[Epoch 0/1] [Batch 431/938] [D loss: 0.468139] [G loss: 0.749367]\n",
            "[Epoch 0/1] [Batch 432/938] [D loss: 0.440605] [G loss: 1.127482]\n",
            "[Epoch 0/1] [Batch 433/938] [D loss: 0.466003] [G loss: 0.645332]\n",
            "[Epoch 0/1] [Batch 434/938] [D loss: 0.505181] [G loss: 1.537215]\n",
            "[Epoch 0/1] [Batch 435/938] [D loss: 0.636840] [G loss: 0.389069]\n",
            "[Epoch 0/1] [Batch 436/938] [D loss: 0.453693] [G loss: 1.420740]\n",
            "[Epoch 0/1] [Batch 437/938] [D loss: 0.445540] [G loss: 0.739435]\n",
            "[Epoch 0/1] [Batch 438/938] [D loss: 0.417464] [G loss: 0.853203]\n",
            "[Epoch 0/1] [Batch 439/938] [D loss: 0.446382] [G loss: 1.097463]\n",
            "[Epoch 0/1] [Batch 440/938] [D loss: 0.485186] [G loss: 0.628603]\n",
            "[Epoch 0/1] [Batch 441/938] [D loss: 0.501953] [G loss: 1.286740]\n",
            "[Epoch 0/1] [Batch 442/938] [D loss: 0.541225] [G loss: 0.518388]\n",
            "[Epoch 0/1] [Batch 443/938] [D loss: 0.492981] [G loss: 1.255092]\n",
            "[Epoch 0/1] [Batch 444/938] [D loss: 0.469736] [G loss: 0.699351]\n",
            "[Epoch 0/1] [Batch 445/938] [D loss: 0.418068] [G loss: 1.007779]\n",
            "[Epoch 0/1] [Batch 446/938] [D loss: 0.437238] [G loss: 0.995352]\n",
            "[Epoch 0/1] [Batch 447/938] [D loss: 0.426602] [G loss: 0.853836]\n",
            "[Epoch 0/1] [Batch 448/938] [D loss: 0.503344] [G loss: 1.100904]\n",
            "[Epoch 0/1] [Batch 449/938] [D loss: 0.525148] [G loss: 0.607197]\n",
            "[Epoch 0/1] [Batch 450/938] [D loss: 0.528911] [G loss: 1.554784]\n",
            "[Epoch 0/1] [Batch 451/938] [D loss: 0.607348] [G loss: 0.445225]\n",
            "[Epoch 0/1] [Batch 452/938] [D loss: 0.482476] [G loss: 1.595620]\n",
            "[Epoch 0/1] [Batch 453/938] [D loss: 0.446046] [G loss: 0.742152]\n",
            "[Epoch 0/1] [Batch 454/938] [D loss: 0.452189] [G loss: 1.233892]\n",
            "[Epoch 0/1] [Batch 455/938] [D loss: 0.434668] [G loss: 0.868085]\n",
            "[Epoch 0/1] [Batch 456/938] [D loss: 0.465366] [G loss: 1.184529]\n",
            "[Epoch 0/1] [Batch 457/938] [D loss: 0.482012] [G loss: 0.781563]\n",
            "[Epoch 0/1] [Batch 458/938] [D loss: 0.518533] [G loss: 1.355149]\n",
            "[Epoch 0/1] [Batch 459/938] [D loss: 0.635659] [G loss: 0.445992]\n",
            "[Epoch 0/1] [Batch 460/938] [D loss: 0.735725] [G loss: 2.007756]\n",
            "[Epoch 0/1] [Batch 461/938] [D loss: 0.917241] [G loss: 0.214627]\n",
            "[Epoch 0/1] [Batch 462/938] [D loss: 0.560553] [G loss: 1.321092]\n",
            "[Epoch 0/1] [Batch 463/938] [D loss: 0.489487] [G loss: 0.879833]\n",
            "[Epoch 0/1] [Batch 464/938] [D loss: 0.534931] [G loss: 0.818238]\n",
            "[Epoch 0/1] [Batch 465/938] [D loss: 0.508734] [G loss: 0.888159]\n",
            "[Epoch 0/1] [Batch 466/938] [D loss: 0.502732] [G loss: 0.886322]\n",
            "[Epoch 0/1] [Batch 467/938] [D loss: 0.515714] [G loss: 0.895748]\n",
            "[Epoch 0/1] [Batch 468/938] [D loss: 0.495646] [G loss: 0.919480]\n",
            "[Epoch 0/1] [Batch 469/938] [D loss: 0.512491] [G loss: 0.842923]\n",
            "[Epoch 0/1] [Batch 470/938] [D loss: 0.489951] [G loss: 0.945032]\n",
            "[Epoch 0/1] [Batch 471/938] [D loss: 0.474626] [G loss: 0.838697]\n",
            "[Epoch 0/1] [Batch 472/938] [D loss: 0.484172] [G loss: 1.106360]\n",
            "[Epoch 0/1] [Batch 473/938] [D loss: 0.520773] [G loss: 0.658687]\n",
            "[Epoch 0/1] [Batch 474/938] [D loss: 0.573466] [G loss: 1.524697]\n",
            "[Epoch 0/1] [Batch 475/938] [D loss: 0.830797] [G loss: 0.254448]\n",
            "[Epoch 0/1] [Batch 476/938] [D loss: 0.709962] [G loss: 1.975972]\n",
            "[Epoch 0/1] [Batch 477/938] [D loss: 0.643843] [G loss: 0.386622]\n",
            "[Epoch 0/1] [Batch 478/938] [D loss: 0.432259] [G loss: 1.269769]\n",
            "[Epoch 0/1] [Batch 479/938] [D loss: 0.464033] [G loss: 1.152362]\n",
            "[Epoch 0/1] [Batch 480/938] [D loss: 0.540349] [G loss: 0.549073]\n",
            "[Epoch 0/1] [Batch 481/938] [D loss: 0.523889] [G loss: 1.425560]\n",
            "[Epoch 0/1] [Batch 482/938] [D loss: 0.552485] [G loss: 0.568177]\n",
            "[Epoch 0/1] [Batch 483/938] [D loss: 0.510809] [G loss: 1.264764]\n",
            "[Epoch 0/1] [Batch 484/938] [D loss: 0.524349] [G loss: 0.598036]\n",
            "[Epoch 0/1] [Batch 485/938] [D loss: 0.501544] [G loss: 1.346421]\n",
            "[Epoch 0/1] [Batch 486/938] [D loss: 0.492961] [G loss: 0.653861]\n",
            "[Epoch 0/1] [Batch 487/938] [D loss: 0.448899] [G loss: 1.378474]\n",
            "[Epoch 0/1] [Batch 488/938] [D loss: 0.435680] [G loss: 0.776583]\n",
            "[Epoch 0/1] [Batch 489/938] [D loss: 0.423755] [G loss: 1.271139]\n",
            "[Epoch 0/1] [Batch 490/938] [D loss: 0.446414] [G loss: 0.780232]\n",
            "[Epoch 0/1] [Batch 491/938] [D loss: 0.476281] [G loss: 1.301014]\n",
            "[Epoch 0/1] [Batch 492/938] [D loss: 0.583782] [G loss: 0.508912]\n",
            "[Epoch 0/1] [Batch 493/938] [D loss: 0.595569] [G loss: 1.630580]\n",
            "[Epoch 0/1] [Batch 494/938] [D loss: 0.607629] [G loss: 0.463237]\n",
            "[Epoch 0/1] [Batch 495/938] [D loss: 0.565951] [G loss: 1.275502]\n",
            "[Epoch 0/1] [Batch 496/938] [D loss: 0.514042] [G loss: 0.585834]\n",
            "[Epoch 0/1] [Batch 497/938] [D loss: 0.466325] [G loss: 1.377867]\n",
            "[Epoch 0/1] [Batch 498/938] [D loss: 0.449091] [G loss: 0.858357]\n",
            "[Epoch 0/1] [Batch 499/938] [D loss: 0.386994] [G loss: 0.974562]\n",
            "[Epoch 0/1] [Batch 500/938] [D loss: 0.437796] [G loss: 1.334569]\n",
            "[Epoch 0/1] [Batch 501/938] [D loss: 0.460768] [G loss: 0.670863]\n",
            "[Epoch 0/1] [Batch 502/938] [D loss: 0.497928] [G loss: 1.472413]\n",
            "[Epoch 0/1] [Batch 503/938] [D loss: 0.498805] [G loss: 0.653086]\n",
            "[Epoch 0/1] [Batch 504/938] [D loss: 0.441408] [G loss: 1.328235]\n",
            "[Epoch 0/1] [Batch 505/938] [D loss: 0.457493] [G loss: 0.826842]\n",
            "[Epoch 0/1] [Batch 506/938] [D loss: 0.422536] [G loss: 1.097463]\n",
            "[Epoch 0/1] [Batch 507/938] [D loss: 0.447340] [G loss: 0.966230]\n",
            "[Epoch 0/1] [Batch 508/938] [D loss: 0.491359] [G loss: 0.934991]\n",
            "[Epoch 0/1] [Batch 509/938] [D loss: 0.488904] [G loss: 0.773750]\n",
            "[Epoch 0/1] [Batch 510/938] [D loss: 0.524575] [G loss: 1.363404]\n",
            "[Epoch 0/1] [Batch 511/938] [D loss: 0.685108] [G loss: 0.403333]\n",
            "[Epoch 0/1] [Batch 512/938] [D loss: 0.647564] [G loss: 1.577822]\n",
            "[Epoch 0/1] [Batch 513/938] [D loss: 0.676015] [G loss: 0.391789]\n",
            "[Epoch 0/1] [Batch 514/938] [D loss: 0.503555] [G loss: 1.318069]\n",
            "[Epoch 0/1] [Batch 515/938] [D loss: 0.468145] [G loss: 0.769431]\n",
            "[Epoch 0/1] [Batch 516/938] [D loss: 0.424604] [G loss: 1.063072]\n",
            "[Epoch 0/1] [Batch 517/938] [D loss: 0.393751] [G loss: 1.026491]\n",
            "[Epoch 0/1] [Batch 518/938] [D loss: 0.393950] [G loss: 1.075765]\n",
            "[Epoch 0/1] [Batch 519/938] [D loss: 0.408721] [G loss: 1.072263]\n",
            "[Epoch 0/1] [Batch 520/938] [D loss: 0.428928] [G loss: 1.014739]\n",
            "[Epoch 0/1] [Batch 521/938] [D loss: 0.404805] [G loss: 1.057074]\n",
            "[Epoch 0/1] [Batch 522/938] [D loss: 0.396972] [G loss: 1.032122]\n",
            "[Epoch 0/1] [Batch 523/938] [D loss: 0.402055] [G loss: 1.048370]\n",
            "[Epoch 0/1] [Batch 524/938] [D loss: 0.410725] [G loss: 1.089445]\n",
            "[Epoch 0/1] [Batch 525/938] [D loss: 0.391315] [G loss: 1.031517]\n",
            "[Epoch 0/1] [Batch 526/938] [D loss: 0.386112] [G loss: 1.106727]\n",
            "[Epoch 0/1] [Batch 527/938] [D loss: 0.394276] [G loss: 1.102185]\n",
            "[Epoch 0/1] [Batch 528/938] [D loss: 0.406028] [G loss: 1.051787]\n",
            "[Epoch 0/1] [Batch 529/938] [D loss: 0.404492] [G loss: 1.023990]\n",
            "[Epoch 0/1] [Batch 530/938] [D loss: 0.436779] [G loss: 1.131983]\n",
            "[Epoch 0/1] [Batch 531/938] [D loss: 0.449227] [G loss: 0.760762]\n",
            "[Epoch 0/1] [Batch 532/938] [D loss: 0.426748] [G loss: 1.629534]\n",
            "[Epoch 0/1] [Batch 533/938] [D loss: 0.495209] [G loss: 0.585774]\n",
            "[Epoch 0/1] [Batch 534/938] [D loss: 0.468864] [G loss: 1.824802]\n",
            "[Epoch 0/1] [Batch 535/938] [D loss: 0.460721] [G loss: 0.617070]\n",
            "[Epoch 0/1] [Batch 536/938] [D loss: 0.410535] [G loss: 1.661696]\n",
            "[Epoch 0/1] [Batch 537/938] [D loss: 0.394110] [G loss: 0.788947]\n",
            "[Epoch 0/1] [Batch 538/938] [D loss: 0.373366] [G loss: 1.428889]\n",
            "[Epoch 0/1] [Batch 539/938] [D loss: 0.373114] [G loss: 1.001664]\n",
            "[Epoch 0/1] [Batch 540/938] [D loss: 0.349206] [G loss: 1.247719]\n",
            "[Epoch 0/1] [Batch 541/938] [D loss: 0.346043] [G loss: 1.088714]\n",
            "[Epoch 0/1] [Batch 542/938] [D loss: 0.350267] [G loss: 1.374742]\n",
            "[Epoch 0/1] [Batch 543/938] [D loss: 0.426158] [G loss: 0.812330]\n",
            "[Epoch 0/1] [Batch 544/938] [D loss: 0.497512] [G loss: 1.759870]\n",
            "[Epoch 0/1] [Batch 545/938] [D loss: 0.650144] [G loss: 0.363358]\n",
            "[Epoch 0/1] [Batch 546/938] [D loss: 0.506021] [G loss: 2.315166]\n",
            "[Epoch 0/1] [Batch 547/938] [D loss: 0.416217] [G loss: 0.725511]\n",
            "[Epoch 0/1] [Batch 548/938] [D loss: 0.322229] [G loss: 1.336518]\n",
            "[Epoch 0/1] [Batch 549/938] [D loss: 0.345205] [G loss: 1.246573]\n",
            "[Epoch 0/1] [Batch 550/938] [D loss: 0.387314] [G loss: 0.962822]\n",
            "[Epoch 0/1] [Batch 551/938] [D loss: 0.423943] [G loss: 1.379800]\n",
            "[Epoch 0/1] [Batch 552/938] [D loss: 0.513228] [G loss: 0.598226]\n",
            "[Epoch 0/1] [Batch 553/938] [D loss: 0.594673] [G loss: 2.050448]\n",
            "[Epoch 0/1] [Batch 554/938] [D loss: 0.712944] [G loss: 0.303972]\n",
            "[Epoch 0/1] [Batch 555/938] [D loss: 0.495100] [G loss: 2.266043]\n",
            "[Epoch 0/1] [Batch 556/938] [D loss: 0.392337] [G loss: 0.859825]\n",
            "[Epoch 0/1] [Batch 557/938] [D loss: 0.375388] [G loss: 0.993803]\n",
            "[Epoch 0/1] [Batch 558/938] [D loss: 0.385013] [G loss: 1.590844]\n",
            "[Epoch 0/1] [Batch 559/938] [D loss: 0.428008] [G loss: 0.766825]\n",
            "[Epoch 0/1] [Batch 560/938] [D loss: 0.444476] [G loss: 1.652565]\n",
            "[Epoch 0/1] [Batch 561/938] [D loss: 0.507209] [G loss: 0.584923]\n",
            "[Epoch 0/1] [Batch 562/938] [D loss: 0.476180] [G loss: 1.839740]\n",
            "[Epoch 0/1] [Batch 563/938] [D loss: 0.473488] [G loss: 0.609440]\n",
            "[Epoch 0/1] [Batch 564/938] [D loss: 0.387703] [G loss: 1.829828]\n",
            "[Epoch 0/1] [Batch 565/938] [D loss: 0.379625] [G loss: 0.904676]\n",
            "[Epoch 0/1] [Batch 566/938] [D loss: 0.361328] [G loss: 1.340312]\n",
            "[Epoch 0/1] [Batch 567/938] [D loss: 0.374839] [G loss: 1.045649]\n",
            "[Epoch 0/1] [Batch 568/938] [D loss: 0.377649] [G loss: 1.207200]\n",
            "[Epoch 0/1] [Batch 569/938] [D loss: 0.395065] [G loss: 1.026920]\n",
            "[Epoch 0/1] [Batch 570/938] [D loss: 0.429287] [G loss: 1.235405]\n",
            "[Epoch 0/1] [Batch 571/938] [D loss: 0.436903] [G loss: 0.755209]\n",
            "[Epoch 0/1] [Batch 572/938] [D loss: 0.422650] [G loss: 2.072445]\n",
            "[Epoch 0/1] [Batch 573/938] [D loss: 0.501022] [G loss: 0.551213]\n",
            "[Epoch 0/1] [Batch 574/938] [D loss: 0.519896] [G loss: 2.557528]\n",
            "[Epoch 0/1] [Batch 575/938] [D loss: 0.531166] [G loss: 0.537358]\n",
            "[Epoch 0/1] [Batch 576/938] [D loss: 0.360997] [G loss: 1.929843]\n",
            "[Epoch 0/1] [Batch 577/938] [D loss: 0.322778] [G loss: 1.092352]\n",
            "[Epoch 0/1] [Batch 578/938] [D loss: 0.331175] [G loss: 1.277966]\n",
            "[Epoch 0/1] [Batch 579/938] [D loss: 0.346153] [G loss: 1.158344]\n",
            "[Epoch 0/1] [Batch 580/938] [D loss: 0.357527] [G loss: 1.292303]\n",
            "[Epoch 0/1] [Batch 581/938] [D loss: 0.378798] [G loss: 1.006386]\n",
            "[Epoch 0/1] [Batch 582/938] [D loss: 0.380766] [G loss: 1.583832]\n",
            "[Epoch 0/1] [Batch 583/938] [D loss: 0.502808] [G loss: 0.563637]\n",
            "[Epoch 0/1] [Batch 584/938] [D loss: 0.754895] [G loss: 2.738248]\n",
            "[Epoch 0/1] [Batch 585/938] [D loss: 0.967971] [G loss: 0.169318]\n",
            "[Epoch 0/1] [Batch 586/938] [D loss: 0.502473] [G loss: 2.327214]\n",
            "[Epoch 0/1] [Batch 587/938] [D loss: 0.372427] [G loss: 1.031015]\n",
            "[Epoch 0/1] [Batch 588/938] [D loss: 0.348006] [G loss: 0.953335]\n",
            "[Epoch 0/1] [Batch 589/938] [D loss: 0.390317] [G loss: 1.721356]\n",
            "[Epoch 0/1] [Batch 590/938] [D loss: 0.449892] [G loss: 0.767823]\n",
            "[Epoch 0/1] [Batch 591/938] [D loss: 0.404576] [G loss: 1.375511]\n",
            "[Epoch 0/1] [Batch 592/938] [D loss: 0.482353] [G loss: 0.932494]\n",
            "[Epoch 0/1] [Batch 593/938] [D loss: 0.450468] [G loss: 0.870053]\n",
            "[Epoch 0/1] [Batch 594/938] [D loss: 0.480094] [G loss: 1.436886]\n",
            "[Epoch 0/1] [Batch 595/938] [D loss: 0.555654] [G loss: 0.533530]\n",
            "[Epoch 0/1] [Batch 596/938] [D loss: 0.494196] [G loss: 1.994828]\n",
            "[Epoch 0/1] [Batch 597/938] [D loss: 0.497881] [G loss: 0.572194]\n",
            "[Epoch 0/1] [Batch 598/938] [D loss: 0.419623] [G loss: 1.754402]\n",
            "[Epoch 0/1] [Batch 599/938] [D loss: 0.395920] [G loss: 0.804545]\n",
            "[Epoch 0/1] [Batch 600/938] [D loss: 0.407845] [G loss: 1.475991]\n",
            "[Epoch 0/1] [Batch 601/938] [D loss: 0.431277] [G loss: 0.770289]\n",
            "[Epoch 0/1] [Batch 602/938] [D loss: 0.388717] [G loss: 1.574996]\n",
            "[Epoch 0/1] [Batch 603/938] [D loss: 0.446476] [G loss: 0.789982]\n",
            "[Epoch 0/1] [Batch 604/938] [D loss: 0.437682] [G loss: 1.420593]\n",
            "[Epoch 0/1] [Batch 605/938] [D loss: 0.508688] [G loss: 0.616043]\n",
            "[Epoch 0/1] [Batch 606/938] [D loss: 0.416018] [G loss: 1.780218]\n",
            "[Epoch 0/1] [Batch 607/938] [D loss: 0.444743] [G loss: 0.692291]\n",
            "[Epoch 0/1] [Batch 608/938] [D loss: 0.471629] [G loss: 1.559624]\n",
            "[Epoch 0/1] [Batch 609/938] [D loss: 0.509177] [G loss: 0.562237]\n",
            "[Epoch 0/1] [Batch 610/938] [D loss: 0.440472] [G loss: 1.967953]\n",
            "[Epoch 0/1] [Batch 611/938] [D loss: 0.412256] [G loss: 0.745738]\n",
            "[Epoch 0/1] [Batch 612/938] [D loss: 0.382211] [G loss: 1.489293]\n",
            "[Epoch 0/1] [Batch 613/938] [D loss: 0.376959] [G loss: 0.926073]\n",
            "[Epoch 0/1] [Batch 614/938] [D loss: 0.387968] [G loss: 1.499160]\n",
            "[Epoch 0/1] [Batch 615/938] [D loss: 0.392953] [G loss: 0.837603]\n",
            "[Epoch 0/1] [Batch 616/938] [D loss: 0.409858] [G loss: 1.599220]\n",
            "[Epoch 0/1] [Batch 617/938] [D loss: 0.426946] [G loss: 0.733444]\n",
            "[Epoch 0/1] [Batch 618/938] [D loss: 0.348442] [G loss: 1.765845]\n",
            "[Epoch 0/1] [Batch 619/938] [D loss: 0.386968] [G loss: 0.909303]\n",
            "[Epoch 0/1] [Batch 620/938] [D loss: 0.341483] [G loss: 1.224588]\n",
            "[Epoch 0/1] [Batch 621/938] [D loss: 0.349823] [G loss: 1.305187]\n",
            "[Epoch 0/1] [Batch 622/938] [D loss: 0.385125] [G loss: 0.984783]\n",
            "[Epoch 0/1] [Batch 623/938] [D loss: 0.463151] [G loss: 1.575923]\n",
            "[Epoch 0/1] [Batch 624/938] [D loss: 0.639295] [G loss: 0.437428]\n",
            "[Epoch 0/1] [Batch 625/938] [D loss: 0.683321] [G loss: 2.611700]\n",
            "[Epoch 0/1] [Batch 626/938] [D loss: 0.586197] [G loss: 0.461770]\n",
            "[Epoch 0/1] [Batch 627/938] [D loss: 0.328421] [G loss: 1.550143]\n",
            "[Epoch 0/1] [Batch 628/938] [D loss: 0.325185] [G loss: 1.439478]\n",
            "[Epoch 0/1] [Batch 629/938] [D loss: 0.376395] [G loss: 0.908122]\n",
            "[Epoch 0/1] [Batch 630/938] [D loss: 0.365110] [G loss: 1.322336]\n",
            "[Epoch 0/1] [Batch 631/938] [D loss: 0.381993] [G loss: 1.011669]\n",
            "[Epoch 0/1] [Batch 632/938] [D loss: 0.413843] [G loss: 1.131767]\n",
            "[Epoch 0/1] [Batch 633/938] [D loss: 0.474609] [G loss: 0.791402]\n",
            "[Epoch 0/1] [Batch 634/938] [D loss: 0.502121] [G loss: 1.325740]\n",
            "[Epoch 0/1] [Batch 635/938] [D loss: 0.554884] [G loss: 0.563239]\n",
            "[Epoch 0/1] [Batch 636/938] [D loss: 0.501319] [G loss: 1.668419]\n",
            "[Epoch 0/1] [Batch 637/938] [D loss: 0.474941] [G loss: 0.624951]\n",
            "[Epoch 0/1] [Batch 638/938] [D loss: 0.350263] [G loss: 1.613440]\n",
            "[Epoch 0/1] [Batch 639/938] [D loss: 0.331338] [G loss: 1.235209]\n",
            "[Epoch 0/1] [Batch 640/938] [D loss: 0.348313] [G loss: 0.924774]\n",
            "[Epoch 0/1] [Batch 641/938] [D loss: 0.325893] [G loss: 1.730362]\n",
            "[Epoch 0/1] [Batch 642/938] [D loss: 0.329155] [G loss: 1.001932]\n",
            "[Epoch 0/1] [Batch 643/938] [D loss: 0.301049] [G loss: 1.423851]\n",
            "[Epoch 0/1] [Batch 644/938] [D loss: 0.315000] [G loss: 1.249818]\n",
            "[Epoch 0/1] [Batch 645/938] [D loss: 0.317333] [G loss: 1.142430]\n",
            "[Epoch 0/1] [Batch 646/938] [D loss: 0.308252] [G loss: 1.484948]\n",
            "[Epoch 0/1] [Batch 647/938] [D loss: 0.326398] [G loss: 0.992249]\n",
            "[Epoch 0/1] [Batch 648/938] [D loss: 0.400404] [G loss: 1.796575]\n",
            "[Epoch 0/1] [Batch 649/938] [D loss: 0.462218] [G loss: 0.583147]\n",
            "[Epoch 0/1] [Batch 650/938] [D loss: 0.422134] [G loss: 2.156167]\n",
            "[Epoch 0/1] [Batch 651/938] [D loss: 0.378490] [G loss: 0.741456]\n",
            "[Epoch 0/1] [Batch 652/938] [D loss: 0.239597] [G loss: 1.515522]\n",
            "[Epoch 0/1] [Batch 653/938] [D loss: 0.228192] [G loss: 1.705295]\n",
            "[Epoch 0/1] [Batch 654/938] [D loss: 0.278324] [G loss: 1.219932]\n",
            "[Epoch 0/1] [Batch 655/938] [D loss: 0.333221] [G loss: 1.210788]\n",
            "[Epoch 0/1] [Batch 656/938] [D loss: 0.291954] [G loss: 1.246657]\n",
            "[Epoch 0/1] [Batch 657/938] [D loss: 0.352157] [G loss: 1.261837]\n",
            "[Epoch 0/1] [Batch 658/938] [D loss: 0.323551] [G loss: 1.014540]\n",
            "[Epoch 0/1] [Batch 659/938] [D loss: 0.344505] [G loss: 1.613606]\n",
            "[Epoch 0/1] [Batch 660/938] [D loss: 0.352184] [G loss: 0.854500]\n",
            "[Epoch 0/1] [Batch 661/938] [D loss: 0.300694] [G loss: 1.683758]\n",
            "[Epoch 0/1] [Batch 662/938] [D loss: 0.259774] [G loss: 1.166191]\n",
            "[Epoch 0/1] [Batch 663/938] [D loss: 0.308262] [G loss: 1.466028]\n",
            "[Epoch 0/1] [Batch 664/938] [D loss: 0.309990] [G loss: 1.129467]\n",
            "[Epoch 0/1] [Batch 665/938] [D loss: 0.271850] [G loss: 1.295474]\n",
            "[Epoch 0/1] [Batch 666/938] [D loss: 0.251958] [G loss: 1.514093]\n",
            "[Epoch 0/1] [Batch 667/938] [D loss: 0.315181] [G loss: 1.184184]\n",
            "[Epoch 0/1] [Batch 668/938] [D loss: 0.301142] [G loss: 1.453363]\n",
            "[Epoch 0/1] [Batch 669/938] [D loss: 0.307465] [G loss: 1.167034]\n",
            "[Epoch 0/1] [Batch 670/938] [D loss: 0.351715] [G loss: 1.334342]\n",
            "[Epoch 0/1] [Batch 671/938] [D loss: 0.345106] [G loss: 0.917791]\n",
            "[Epoch 0/1] [Batch 672/938] [D loss: 0.444320] [G loss: 2.311694]\n",
            "[Epoch 0/1] [Batch 673/938] [D loss: 0.642136] [G loss: 0.389975]\n",
            "[Epoch 0/1] [Batch 674/938] [D loss: 0.533214] [G loss: 2.951707]\n",
            "[Epoch 0/1] [Batch 675/938] [D loss: 0.488561] [G loss: 0.560171]\n",
            "[Epoch 0/1] [Batch 676/938] [D loss: 0.305753] [G loss: 2.124525]\n",
            "[Epoch 0/1] [Batch 677/938] [D loss: 0.325121] [G loss: 1.213535]\n",
            "[Epoch 0/1] [Batch 678/938] [D loss: 0.340305] [G loss: 1.251326]\n",
            "[Epoch 0/1] [Batch 679/938] [D loss: 0.420192] [G loss: 1.378711]\n",
            "[Epoch 0/1] [Batch 680/938] [D loss: 0.535488] [G loss: 0.540922]\n",
            "[Epoch 0/1] [Batch 681/938] [D loss: 0.897034] [G loss: 3.027518]\n",
            "[Epoch 0/1] [Batch 682/938] [D loss: 1.468511] [G loss: 0.060636]\n",
            "[Epoch 0/1] [Batch 683/938] [D loss: 0.455354] [G loss: 1.678625]\n",
            "[Epoch 0/1] [Batch 684/938] [D loss: 0.403399] [G loss: 1.409153]\n",
            "[Epoch 0/1] [Batch 685/938] [D loss: 0.469854] [G loss: 0.637822]\n",
            "[Epoch 0/1] [Batch 686/938] [D loss: 0.418343] [G loss: 1.793178]\n",
            "[Epoch 0/1] [Batch 687/938] [D loss: 0.406126] [G loss: 0.809935]\n",
            "[Epoch 0/1] [Batch 688/938] [D loss: 0.427459] [G loss: 1.568118]\n",
            "[Epoch 0/1] [Batch 689/938] [D loss: 0.449468] [G loss: 0.719370]\n",
            "[Epoch 0/1] [Batch 690/938] [D loss: 0.391889] [G loss: 1.791389]\n",
            "[Epoch 0/1] [Batch 691/938] [D loss: 0.449731] [G loss: 0.790046]\n",
            "[Epoch 0/1] [Batch 692/938] [D loss: 0.411140] [G loss: 1.398081]\n",
            "[Epoch 0/1] [Batch 693/938] [D loss: 0.498683] [G loss: 0.795243]\n",
            "[Epoch 0/1] [Batch 694/938] [D loss: 0.474499] [G loss: 1.249541]\n",
            "[Epoch 0/1] [Batch 695/938] [D loss: 0.527134] [G loss: 0.606841]\n",
            "[Epoch 0/1] [Batch 696/938] [D loss: 0.644350] [G loss: 2.231239]\n",
            "[Epoch 0/1] [Batch 697/938] [D loss: 0.791554] [G loss: 0.302964]\n",
            "[Epoch 0/1] [Batch 698/938] [D loss: 0.478415] [G loss: 1.939122]\n",
            "[Epoch 0/1] [Batch 699/938] [D loss: 0.419476] [G loss: 1.092916]\n",
            "[Epoch 0/1] [Batch 700/938] [D loss: 0.399337] [G loss: 0.976740]\n",
            "[Epoch 0/1] [Batch 701/938] [D loss: 0.383128] [G loss: 1.495232]\n",
            "[Epoch 0/1] [Batch 702/938] [D loss: 0.428221] [G loss: 0.843454]\n",
            "[Epoch 0/1] [Batch 703/938] [D loss: 0.377982] [G loss: 1.409476]\n",
            "[Epoch 0/1] [Batch 704/938] [D loss: 0.370898] [G loss: 1.131782]\n",
            "[Epoch 0/1] [Batch 705/938] [D loss: 0.334337] [G loss: 1.181348]\n",
            "[Epoch 0/1] [Batch 706/938] [D loss: 0.387014] [G loss: 1.527600]\n",
            "[Epoch 0/1] [Batch 707/938] [D loss: 0.471303] [G loss: 0.694016]\n",
            "[Epoch 0/1] [Batch 708/938] [D loss: 0.482554] [G loss: 2.271867]\n",
            "[Epoch 0/1] [Batch 709/938] [D loss: 0.554930] [G loss: 0.545632]\n",
            "[Epoch 0/1] [Batch 710/938] [D loss: 0.525946] [G loss: 2.257452]\n",
            "[Epoch 0/1] [Batch 711/938] [D loss: 0.567189] [G loss: 0.461961]\n",
            "[Epoch 0/1] [Batch 712/938] [D loss: 0.410988] [G loss: 2.351986]\n",
            "[Epoch 0/1] [Batch 713/938] [D loss: 0.323379] [G loss: 1.106314]\n",
            "[Epoch 0/1] [Batch 714/938] [D loss: 0.292190] [G loss: 1.215847]\n",
            "[Epoch 0/1] [Batch 715/938] [D loss: 0.353317] [G loss: 1.739877]\n",
            "[Epoch 0/1] [Batch 716/938] [D loss: 0.427859] [G loss: 0.688719]\n",
            "[Epoch 0/1] [Batch 717/938] [D loss: 0.529718] [G loss: 2.495774]\n",
            "[Epoch 0/1] [Batch 718/938] [D loss: 0.592304] [G loss: 0.417853]\n",
            "[Epoch 0/1] [Batch 719/938] [D loss: 0.381219] [G loss: 2.414577]\n",
            "[Epoch 0/1] [Batch 720/938] [D loss: 0.305168] [G loss: 1.272468]\n",
            "[Epoch 0/1] [Batch 721/938] [D loss: 0.318085] [G loss: 1.068918]\n",
            "[Epoch 0/1] [Batch 722/938] [D loss: 0.339912] [G loss: 1.855192]\n",
            "[Epoch 0/1] [Batch 723/938] [D loss: 0.420199] [G loss: 0.738252]\n",
            "[Epoch 0/1] [Batch 724/938] [D loss: 0.443804] [G loss: 2.073977]\n",
            "[Epoch 0/1] [Batch 725/938] [D loss: 0.546626] [G loss: 0.516141]\n",
            "[Epoch 0/1] [Batch 726/938] [D loss: 0.521548] [G loss: 2.646668]\n",
            "[Epoch 0/1] [Batch 727/938] [D loss: 0.486436] [G loss: 0.619150]\n",
            "[Epoch 0/1] [Batch 728/938] [D loss: 0.341018] [G loss: 2.057724]\n",
            "[Epoch 0/1] [Batch 729/938] [D loss: 0.311278] [G loss: 1.167606]\n",
            "[Epoch 0/1] [Batch 730/938] [D loss: 0.320070] [G loss: 1.300009]\n",
            "[Epoch 0/1] [Batch 731/938] [D loss: 0.320915] [G loss: 1.370709]\n",
            "[Epoch 0/1] [Batch 732/938] [D loss: 0.344477] [G loss: 1.105849]\n",
            "[Epoch 0/1] [Batch 733/938] [D loss: 0.330668] [G loss: 1.497108]\n",
            "[Epoch 0/1] [Batch 734/938] [D loss: 0.431844] [G loss: 0.893579]\n",
            "[Epoch 0/1] [Batch 735/938] [D loss: 0.404453] [G loss: 1.545275]\n",
            "[Epoch 0/1] [Batch 736/938] [D loss: 0.539536] [G loss: 0.539254]\n",
            "[Epoch 0/1] [Batch 737/938] [D loss: 0.687958] [G loss: 2.814213]\n",
            "[Epoch 0/1] [Batch 738/938] [D loss: 0.835684] [G loss: 0.263864]\n",
            "[Epoch 0/1] [Batch 739/938] [D loss: 0.345892] [G loss: 2.415251]\n",
            "[Epoch 0/1] [Batch 740/938] [D loss: 0.323293] [G loss: 1.660777]\n",
            "[Epoch 0/1] [Batch 741/938] [D loss: 0.431110] [G loss: 0.720365]\n",
            "[Epoch 0/1] [Batch 742/938] [D loss: 0.337037] [G loss: 1.750618]\n",
            "[Epoch 0/1] [Batch 743/938] [D loss: 0.374580] [G loss: 1.003893]\n",
            "[Epoch 0/1] [Batch 744/938] [D loss: 0.391067] [G loss: 1.096117]\n",
            "[Epoch 0/1] [Batch 745/938] [D loss: 0.352298] [G loss: 1.119228]\n",
            "[Epoch 0/1] [Batch 746/938] [D loss: 0.417445] [G loss: 1.276049]\n",
            "[Epoch 0/1] [Batch 747/938] [D loss: 0.481001] [G loss: 0.646656]\n",
            "[Epoch 0/1] [Batch 748/938] [D loss: 0.502767] [G loss: 2.003907]\n",
            "[Epoch 0/1] [Batch 749/938] [D loss: 0.618873] [G loss: 0.452787]\n",
            "[Epoch 0/1] [Batch 750/938] [D loss: 0.465514] [G loss: 1.872842]\n",
            "[Epoch 0/1] [Batch 751/938] [D loss: 0.385834] [G loss: 0.746774]\n",
            "[Epoch 0/1] [Batch 752/938] [D loss: 0.326434] [G loss: 1.382700]\n",
            "[Epoch 0/1] [Batch 753/938] [D loss: 0.332299] [G loss: 1.168505]\n",
            "[Epoch 0/1] [Batch 754/938] [D loss: 0.320073] [G loss: 0.998598]\n",
            "[Epoch 0/1] [Batch 755/938] [D loss: 0.333509] [G loss: 1.462291]\n",
            "[Epoch 0/1] [Batch 756/938] [D loss: 0.352418] [G loss: 0.883889]\n",
            "[Epoch 0/1] [Batch 757/938] [D loss: 0.349292] [G loss: 1.430636]\n",
            "[Epoch 0/1] [Batch 758/938] [D loss: 0.330143] [G loss: 0.962377]\n",
            "[Epoch 0/1] [Batch 759/938] [D loss: 0.327392] [G loss: 1.373004]\n",
            "[Epoch 0/1] [Batch 760/938] [D loss: 0.306139] [G loss: 1.008542]\n",
            "[Epoch 0/1] [Batch 761/938] [D loss: 0.287382] [G loss: 1.584203]\n",
            "[Epoch 0/1] [Batch 762/938] [D loss: 0.331218] [G loss: 0.983773]\n",
            "[Epoch 0/1] [Batch 763/938] [D loss: 0.276750] [G loss: 1.355925]\n",
            "[Epoch 0/1] [Batch 764/938] [D loss: 0.267262] [G loss: 1.343697]\n",
            "[Epoch 0/1] [Batch 765/938] [D loss: 0.289663] [G loss: 1.204224]\n",
            "[Epoch 0/1] [Batch 766/938] [D loss: 0.303361] [G loss: 1.300255]\n",
            "[Epoch 0/1] [Batch 767/938] [D loss: 0.293863] [G loss: 1.115514]\n",
            "[Epoch 0/1] [Batch 768/938] [D loss: 0.335450] [G loss: 1.522572]\n",
            "[Epoch 0/1] [Batch 769/938] [D loss: 0.387361] [G loss: 0.788258]\n",
            "[Epoch 0/1] [Batch 770/938] [D loss: 0.390185] [G loss: 2.049400]\n",
            "[Epoch 0/1] [Batch 771/938] [D loss: 0.392890] [G loss: 0.800186]\n",
            "[Epoch 0/1] [Batch 772/938] [D loss: 0.324120] [G loss: 1.821477]\n",
            "[Epoch 0/1] [Batch 773/938] [D loss: 0.301003] [G loss: 1.051690]\n",
            "[Epoch 0/1] [Batch 774/938] [D loss: 0.291974] [G loss: 1.365645]\n",
            "[Epoch 0/1] [Batch 775/938] [D loss: 0.279404] [G loss: 1.216857]\n",
            "[Epoch 0/1] [Batch 776/938] [D loss: 0.322298] [G loss: 1.360339]\n",
            "[Epoch 0/1] [Batch 777/938] [D loss: 0.339535] [G loss: 0.930434]\n",
            "[Epoch 0/1] [Batch 778/938] [D loss: 0.399032] [G loss: 2.019475]\n",
            "[Epoch 0/1] [Batch 779/938] [D loss: 0.550278] [G loss: 0.485577]\n",
            "[Epoch 0/1] [Batch 780/938] [D loss: 0.535534] [G loss: 2.419768]\n",
            "[Epoch 0/1] [Batch 781/938] [D loss: 0.533007] [G loss: 0.480728]\n",
            "[Epoch 0/1] [Batch 782/938] [D loss: 0.293579] [G loss: 2.073096]\n",
            "[Epoch 0/1] [Batch 783/938] [D loss: 0.266744] [G loss: 1.339626]\n",
            "[Epoch 0/1] [Batch 784/938] [D loss: 0.288622] [G loss: 1.059137]\n",
            "[Epoch 0/1] [Batch 785/938] [D loss: 0.250076] [G loss: 1.728311]\n",
            "[Epoch 0/1] [Batch 786/938] [D loss: 0.296568] [G loss: 1.206159]\n",
            "[Epoch 0/1] [Batch 787/938] [D loss: 0.372650] [G loss: 1.192955]\n",
            "[Epoch 0/1] [Batch 788/938] [D loss: 0.364919] [G loss: 1.000620]\n",
            "[Epoch 0/1] [Batch 789/938] [D loss: 0.312859] [G loss: 1.529033]\n",
            "[Epoch 0/1] [Batch 790/938] [D loss: 0.352917] [G loss: 0.941396]\n",
            "[Epoch 0/1] [Batch 791/938] [D loss: 0.392336] [G loss: 1.917329]\n",
            "[Epoch 0/1] [Batch 792/938] [D loss: 0.656475] [G loss: 0.354701]\n",
            "[Epoch 0/1] [Batch 793/938] [D loss: 1.026968] [G loss: 3.708732]\n",
            "[Epoch 0/1] [Batch 794/938] [D loss: 1.042306] [G loss: 0.145156]\n",
            "[Epoch 0/1] [Batch 795/938] [D loss: 0.238799] [G loss: 1.574614]\n",
            "[Epoch 0/1] [Batch 796/938] [D loss: 0.359408] [G loss: 2.625329]\n",
            "[Epoch 0/1] [Batch 797/938] [D loss: 0.346546] [G loss: 0.826257]\n",
            "[Epoch 0/1] [Batch 798/938] [D loss: 0.322162] [G loss: 1.362183]\n",
            "[Epoch 0/1] [Batch 799/938] [D loss: 0.330141] [G loss: 1.307809]\n",
            "[Epoch 0/1] [Batch 800/938] [D loss: 0.422702] [G loss: 0.971486]\n",
            "[Epoch 0/1] [Batch 801/938] [D loss: 0.464136] [G loss: 1.258433]\n",
            "[Epoch 0/1] [Batch 802/938] [D loss: 0.531212] [G loss: 0.665420]\n",
            "[Epoch 0/1] [Batch 803/938] [D loss: 0.519108] [G loss: 1.839692]\n",
            "[Epoch 0/1] [Batch 804/938] [D loss: 0.677099] [G loss: 0.375149]\n",
            "[Epoch 0/1] [Batch 805/938] [D loss: 0.720092] [G loss: 2.784356]\n",
            "[Epoch 0/1] [Batch 806/938] [D loss: 0.639807] [G loss: 0.398810]\n",
            "[Epoch 0/1] [Batch 807/938] [D loss: 0.356186] [G loss: 1.674365]\n",
            "[Epoch 0/1] [Batch 808/938] [D loss: 0.365446] [G loss: 1.515899]\n",
            "[Epoch 0/1] [Batch 809/938] [D loss: 0.489662] [G loss: 0.612965]\n",
            "[Epoch 0/1] [Batch 810/938] [D loss: 0.422619] [G loss: 1.937960]\n",
            "[Epoch 0/1] [Batch 811/938] [D loss: 0.475789] [G loss: 0.718757]\n",
            "[Epoch 0/1] [Batch 812/938] [D loss: 0.482460] [G loss: 1.468029]\n",
            "[Epoch 0/1] [Batch 813/938] [D loss: 0.491633] [G loss: 0.718324]\n",
            "[Epoch 0/1] [Batch 814/938] [D loss: 0.507452] [G loss: 1.558493]\n",
            "[Epoch 0/1] [Batch 815/938] [D loss: 0.533135] [G loss: 0.546407]\n",
            "[Epoch 0/1] [Batch 816/938] [D loss: 0.631459] [G loss: 2.278781]\n",
            "[Epoch 0/1] [Batch 817/938] [D loss: 0.640996] [G loss: 0.414603]\n",
            "[Epoch 0/1] [Batch 818/938] [D loss: 0.445202] [G loss: 1.936031]\n",
            "[Epoch 0/1] [Batch 819/938] [D loss: 0.401141] [G loss: 0.817550]\n",
            "[Epoch 0/1] [Batch 820/938] [D loss: 0.438573] [G loss: 1.319294]\n",
            "[Epoch 0/1] [Batch 821/938] [D loss: 0.442621] [G loss: 0.806524]\n",
            "[Epoch 0/1] [Batch 822/938] [D loss: 0.431602] [G loss: 1.368055]\n",
            "[Epoch 0/1] [Batch 823/938] [D loss: 0.485864] [G loss: 0.710226]\n",
            "[Epoch 0/1] [Batch 824/938] [D loss: 0.600486] [G loss: 1.641391]\n",
            "[Epoch 0/1] [Batch 825/938] [D loss: 0.827745] [G loss: 0.274499]\n",
            "[Epoch 0/1] [Batch 826/938] [D loss: 0.721094] [G loss: 2.209461]\n",
            "[Epoch 0/1] [Batch 827/938] [D loss: 0.601478] [G loss: 0.486938]\n",
            "[Epoch 0/1] [Batch 828/938] [D loss: 0.445022] [G loss: 1.318895]\n",
            "[Epoch 0/1] [Batch 829/938] [D loss: 0.386899] [G loss: 0.997189]\n",
            "[Epoch 0/1] [Batch 830/938] [D loss: 0.381087] [G loss: 1.065604]\n",
            "[Epoch 0/1] [Batch 831/938] [D loss: 0.399398] [G loss: 1.246793]\n",
            "[Epoch 0/1] [Batch 832/938] [D loss: 0.432682] [G loss: 0.844309]\n",
            "[Epoch 0/1] [Batch 833/938] [D loss: 0.433565] [G loss: 1.443045]\n",
            "[Epoch 0/1] [Batch 834/938] [D loss: 0.490417] [G loss: 0.674981]\n",
            "[Epoch 0/1] [Batch 835/938] [D loss: 0.453193] [G loss: 1.660595]\n",
            "[Epoch 0/1] [Batch 836/938] [D loss: 0.476532] [G loss: 0.665709]\n",
            "[Epoch 0/1] [Batch 837/938] [D loss: 0.404609] [G loss: 1.830281]\n",
            "[Epoch 0/1] [Batch 838/938] [D loss: 0.408972] [G loss: 0.775418]\n",
            "[Epoch 0/1] [Batch 839/938] [D loss: 0.364366] [G loss: 1.770055]\n",
            "[Epoch 0/1] [Batch 840/938] [D loss: 0.401221] [G loss: 0.871111]\n",
            "[Epoch 0/1] [Batch 841/938] [D loss: 0.345663] [G loss: 1.445685]\n",
            "[Epoch 0/1] [Batch 842/938] [D loss: 0.347967] [G loss: 1.132740]\n",
            "[Epoch 0/1] [Batch 843/938] [D loss: 0.351934] [G loss: 1.301129]\n",
            "[Epoch 0/1] [Batch 844/938] [D loss: 0.418069] [G loss: 1.076139]\n",
            "[Epoch 0/1] [Batch 845/938] [D loss: 0.448335] [G loss: 1.012919]\n",
            "[Epoch 0/1] [Batch 846/938] [D loss: 0.400701] [G loss: 1.240886]\n",
            "[Epoch 0/1] [Batch 847/938] [D loss: 0.407195] [G loss: 0.910851]\n",
            "[Epoch 0/1] [Batch 848/938] [D loss: 0.463367] [G loss: 1.951205]\n",
            "[Epoch 0/1] [Batch 849/938] [D loss: 0.803159] [G loss: 0.253292]\n",
            "[Epoch 0/1] [Batch 850/938] [D loss: 0.900989] [G loss: 3.570709]\n",
            "[Epoch 0/1] [Batch 851/938] [D loss: 0.537942] [G loss: 0.525908]\n",
            "[Epoch 0/1] [Batch 852/938] [D loss: 0.231267] [G loss: 1.660158]\n",
            "[Epoch 0/1] [Batch 853/938] [D loss: 0.267201] [G loss: 1.972005]\n",
            "[Epoch 0/1] [Batch 854/938] [D loss: 0.331683] [G loss: 0.934808]\n",
            "[Epoch 0/1] [Batch 855/938] [D loss: 0.301626] [G loss: 1.519771]\n",
            "[Epoch 0/1] [Batch 856/938] [D loss: 0.376323] [G loss: 1.120011]\n",
            "[Epoch 0/1] [Batch 857/938] [D loss: 0.379388] [G loss: 1.149540]\n",
            "[Epoch 0/1] [Batch 858/938] [D loss: 0.422822] [G loss: 1.211999]\n",
            "[Epoch 0/1] [Batch 859/938] [D loss: 0.495753] [G loss: 0.717447]\n",
            "[Epoch 0/1] [Batch 860/938] [D loss: 0.479395] [G loss: 1.918962]\n",
            "[Epoch 0/1] [Batch 861/938] [D loss: 0.647572] [G loss: 0.374165]\n",
            "[Epoch 0/1] [Batch 862/938] [D loss: 0.831392] [G loss: 3.041347]\n",
            "[Epoch 0/1] [Batch 863/938] [D loss: 0.619694] [G loss: 0.373665]\n",
            "[Epoch 0/1] [Batch 864/938] [D loss: 0.306712] [G loss: 2.007071]\n",
            "[Epoch 0/1] [Batch 865/938] [D loss: 0.255165] [G loss: 1.593603]\n",
            "[Epoch 0/1] [Batch 866/938] [D loss: 0.260575] [G loss: 1.096467]\n",
            "[Epoch 0/1] [Batch 867/938] [D loss: 0.250766] [G loss: 1.857492]\n",
            "[Epoch 0/1] [Batch 868/938] [D loss: 0.269967] [G loss: 1.422147]\n",
            "[Epoch 0/1] [Batch 869/938] [D loss: 0.293048] [G loss: 1.211497]\n",
            "[Epoch 0/1] [Batch 870/938] [D loss: 0.353563] [G loss: 1.484458]\n",
            "[Epoch 0/1] [Batch 871/938] [D loss: 0.383166] [G loss: 0.869129]\n",
            "[Epoch 0/1] [Batch 872/938] [D loss: 0.413940] [G loss: 2.002355]\n",
            "[Epoch 0/1] [Batch 873/938] [D loss: 0.530043] [G loss: 0.507166]\n",
            "[Epoch 0/1] [Batch 874/938] [D loss: 0.659721] [G loss: 2.858634]\n",
            "[Epoch 0/1] [Batch 875/938] [D loss: 0.612331] [G loss: 0.387920]\n",
            "[Epoch 0/1] [Batch 876/938] [D loss: 0.353598] [G loss: 2.433798]\n",
            "[Epoch 0/1] [Batch 877/938] [D loss: 0.289165] [G loss: 1.279172]\n",
            "[Epoch 0/1] [Batch 878/938] [D loss: 0.326594] [G loss: 0.948092]\n",
            "[Epoch 0/1] [Batch 879/938] [D loss: 0.370728] [G loss: 1.973294]\n",
            "[Epoch 0/1] [Batch 880/938] [D loss: 0.417314] [G loss: 0.707008]\n",
            "[Epoch 0/1] [Batch 881/938] [D loss: 0.379182] [G loss: 1.857487]\n",
            "[Epoch 0/1] [Batch 882/938] [D loss: 0.460919] [G loss: 0.707388]\n",
            "[Epoch 0/1] [Batch 883/938] [D loss: 0.484459] [G loss: 1.578305]\n",
            "[Epoch 0/1] [Batch 884/938] [D loss: 0.524999] [G loss: 0.578487]\n",
            "[Epoch 0/1] [Batch 885/938] [D loss: 0.472897] [G loss: 2.037312]\n",
            "[Epoch 0/1] [Batch 886/938] [D loss: 0.441262] [G loss: 0.671486]\n",
            "[Epoch 0/1] [Batch 887/938] [D loss: 0.382863] [G loss: 1.790208]\n",
            "[Epoch 0/1] [Batch 888/938] [D loss: 0.363328] [G loss: 0.937009]\n",
            "[Epoch 0/1] [Batch 889/938] [D loss: 0.325857] [G loss: 1.473420]\n",
            "[Epoch 0/1] [Batch 890/938] [D loss: 0.397498] [G loss: 1.075790]\n",
            "[Epoch 0/1] [Batch 891/938] [D loss: 0.355271] [G loss: 1.031873]\n",
            "[Epoch 0/1] [Batch 892/938] [D loss: 0.389131] [G loss: 1.793674]\n",
            "[Epoch 0/1] [Batch 893/938] [D loss: 0.484432] [G loss: 0.604207]\n",
            "[Epoch 0/1] [Batch 894/938] [D loss: 0.477569] [G loss: 2.593241]\n",
            "[Epoch 0/1] [Batch 895/938] [D loss: 0.477207] [G loss: 0.584828]\n",
            "[Epoch 0/1] [Batch 896/938] [D loss: 0.332929] [G loss: 2.361145]\n",
            "[Epoch 0/1] [Batch 897/938] [D loss: 0.336067] [G loss: 1.267195]\n",
            "[Epoch 0/1] [Batch 898/938] [D loss: 0.367540] [G loss: 0.919630]\n",
            "[Epoch 0/1] [Batch 899/938] [D loss: 0.379544] [G loss: 2.112159]\n",
            "[Epoch 0/1] [Batch 900/938] [D loss: 0.458379] [G loss: 0.627077]\n",
            "[Epoch 0/1] [Batch 901/938] [D loss: 0.432446] [G loss: 2.365912]\n",
            "[Epoch 0/1] [Batch 902/938] [D loss: 0.431748] [G loss: 0.661441]\n",
            "[Epoch 0/1] [Batch 903/938] [D loss: 0.436780] [G loss: 2.148248]\n",
            "[Epoch 0/1] [Batch 904/938] [D loss: 0.523969] [G loss: 0.546664]\n",
            "[Epoch 0/1] [Batch 905/938] [D loss: 0.440991] [G loss: 2.256896]\n",
            "[Epoch 0/1] [Batch 906/938] [D loss: 0.520008] [G loss: 0.535532]\n",
            "[Epoch 0/1] [Batch 907/938] [D loss: 0.565343] [G loss: 2.342525]\n",
            "[Epoch 0/1] [Batch 908/938] [D loss: 0.672741] [G loss: 0.382225]\n",
            "[Epoch 0/1] [Batch 909/938] [D loss: 0.591929] [G loss: 2.543960]\n",
            "[Epoch 0/1] [Batch 910/938] [D loss: 0.530912] [G loss: 0.541263]\n",
            "[Epoch 0/1] [Batch 911/938] [D loss: 0.369442] [G loss: 1.432287]\n",
            "[Epoch 0/1] [Batch 912/938] [D loss: 0.380065] [G loss: 1.181989]\n",
            "[Epoch 0/1] [Batch 913/938] [D loss: 0.398246] [G loss: 0.903283]\n",
            "[Epoch 0/1] [Batch 914/938] [D loss: 0.458423] [G loss: 1.624363]\n",
            "[Epoch 0/1] [Batch 915/938] [D loss: 0.531355] [G loss: 0.530272]\n",
            "[Epoch 0/1] [Batch 916/938] [D loss: 0.617965] [G loss: 2.051725]\n",
            "[Epoch 0/1] [Batch 917/938] [D loss: 0.735140] [G loss: 0.311169]\n",
            "[Epoch 0/1] [Batch 918/938] [D loss: 0.606799] [G loss: 2.174736]\n",
            "[Epoch 0/1] [Batch 919/938] [D loss: 0.495531] [G loss: 0.569825]\n",
            "[Epoch 0/1] [Batch 920/938] [D loss: 0.361116] [G loss: 1.368404]\n",
            "[Epoch 0/1] [Batch 921/938] [D loss: 0.373577] [G loss: 1.249331]\n",
            "[Epoch 0/1] [Batch 922/938] [D loss: 0.420231] [G loss: 0.818120]\n",
            "[Epoch 0/1] [Batch 923/938] [D loss: 0.419345] [G loss: 1.444013]\n",
            "[Epoch 0/1] [Batch 924/938] [D loss: 0.437694] [G loss: 0.805765]\n",
            "[Epoch 0/1] [Batch 925/938] [D loss: 0.407693] [G loss: 1.138601]\n",
            "[Epoch 0/1] [Batch 926/938] [D loss: 0.457481] [G loss: 1.118973]\n",
            "[Epoch 0/1] [Batch 927/938] [D loss: 0.478844] [G loss: 0.680520]\n",
            "[Epoch 0/1] [Batch 928/938] [D loss: 0.492267] [G loss: 1.787673]\n",
            "[Epoch 0/1] [Batch 929/938] [D loss: 0.475017] [G loss: 0.623708]\n",
            "[Epoch 0/1] [Batch 930/938] [D loss: 0.421155] [G loss: 1.508077]\n",
            "[Epoch 0/1] [Batch 931/938] [D loss: 0.357997] [G loss: 0.980361]\n",
            "[Epoch 0/1] [Batch 932/938] [D loss: 0.362575] [G loss: 1.198231]\n",
            "[Epoch 0/1] [Batch 933/938] [D loss: 0.403362] [G loss: 1.124447]\n",
            "[Epoch 0/1] [Batch 934/938] [D loss: 0.455369] [G loss: 0.831497]\n",
            "[Epoch 0/1] [Batch 935/938] [D loss: 0.386182] [G loss: 1.285871]\n",
            "[Epoch 0/1] [Batch 936/938] [D loss: 0.355819] [G loss: 1.030052]\n",
            "[Epoch 0/1] [Batch 937/938] [D loss: 0.423300] [G loss: 1.288133]\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "# https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "\n",
        "n_epochs = 1\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_cpu = 8\n",
        "latent_dim = 100\n",
        "img_size = 28\n",
        "channels = 1\n",
        "sample_interval = 400\n",
        "\n",
        "\n",
        "img_shape = (channels, img_size, img_size)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.size(0), *img_shape)\n",
        "        return img\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "\n",
        "        return validity\n",
        "\n",
        "\n",
        "# Loss function\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "\n",
        "# Configure data loader\n",
        "os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\n",
        "        \"../../data/mnist\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transforms.Compose(\n",
        "            [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
        "        ),\n",
        "    ),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
        "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        gen_imgs = generator(z)\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        print(\n",
        "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "            % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
        "        )\n",
        "\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        if batches_done % sample_interval == 0:\n",
        "            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8f3DIwGT0Je"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "lst = list(dataloader)\n",
        "\n",
        "print(len(lst))\n",
        "print(lst[0][0].shape)\n",
        "print(lst[0][1].shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implementing to sample from 1D Gaussian**"
      ],
      "metadata": {
        "id": "BZhucnsVvXLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "os.makedirs(\"samples\", exist_ok=True)\n",
        "\n",
        "n_epochs = 200\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_cpu = 8\n",
        "latent_dim = 100\n",
        "sample_interval = 400\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.model(z)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(1, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        validity = self.model(x)\n",
        "        return validity\n",
        "\n",
        "\n",
        "class Bimodal_1D():\n",
        "  def __init__(self, m1, s1, m2, s2, w1):\n",
        "    self.m1 = m1\n",
        "    self.m2 = m2\n",
        "    self.s1 = s1\n",
        "    self.s2 = s2\n",
        "    self.w1 = w1\n",
        "\n",
        "  def sample(self, n_samples):\n",
        "    mode = np.random.choice([0, 1], size=n_samples, p=[self.w1, 1 - self.w1])\n",
        "    samples1 = np.random.normal(self.m1, self.s1, size=np.sum(mode == 0))\n",
        "    samples2 = np.random.normal(self.m2, self.s2, size=np.sum(mode == 1))\n",
        "\n",
        "    samples = np.concatenate([samples1, samples2])\n",
        "    samples = np.reshape(samples, (n_samples, 1))\n",
        "\n",
        "    return samples\n",
        "\n",
        "\n",
        "# Loss function\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "\n",
        "\n",
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "m1 = 4\n",
        "s1 = 1\n",
        "m2 = -2\n",
        "s2 = 1\n",
        "w1 = 0.7\n",
        "\n",
        "distr = Bimodal_1D(m1, s1, m2, s2, w1)\n",
        "\n",
        "\n",
        "num_iters = 100\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
        "        fake = Variable(Tensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
        "\n",
        "        # Generate a batch of samples\n",
        "        gen_samples = generator(z)\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Measure discriminator's ability to distinguish real samples\n",
        "\n",
        "\n",
        "        #real_samples = Variable(Tensor(np.random.normal(0, 1, (batch_size, 1))))\n",
        "\n",
        "        data = distr.sample(batch_size)\n",
        "\n",
        "        real_samples = Variable(Tensor(data))\n",
        "        real_loss = adversarial_loss(discriminator(real_samples), valid)\n",
        "\n",
        "        # Measure discriminator's ability to distinguish generated samples\n",
        "        fake_loss = adversarial_loss(discriminator(gen_samples.detach()), fake)\n",
        "\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        g_loss = adversarial_loss(discriminator(gen_samples), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        if i % num_iters == 0:\n",
        "            print(\n",
        "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "                % (epoch, n_epochs, i, num_iters, d_loss.item(), g_loss.item())\n",
        "            )\n",
        "\n",
        "    # Generate and save samples\n",
        "    if epoch % 10 == 0:\n",
        "        with torch.no_grad():\n",
        "            z = Variable(Tensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
        "            gen_samples = generator(z)\n",
        "            np.save(f\"samples/epoch_{epoch}.npy\", gen_samples.cpu().numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YyCNMfZnGZnF",
        "outputId": "a03d3984-f943-4af3-ab39-b315b66d3cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0/200] [Batch 0/100] [D loss: 0.620614] [G loss: 0.599132]\n",
            "[Epoch 1/200] [Batch 0/100] [D loss: 0.548455] [G loss: 1.051520]\n",
            "[Epoch 2/200] [Batch 0/100] [D loss: 0.605062] [G loss: 0.911526]\n",
            "[Epoch 3/200] [Batch 0/100] [D loss: 0.628432] [G loss: 0.784581]\n",
            "[Epoch 4/200] [Batch 0/100] [D loss: 0.572081] [G loss: 0.835022]\n",
            "[Epoch 5/200] [Batch 0/100] [D loss: 0.598514] [G loss: 0.800371]\n",
            "[Epoch 6/200] [Batch 0/100] [D loss: 0.585737] [G loss: 0.905617]\n",
            "[Epoch 7/200] [Batch 0/100] [D loss: 0.571647] [G loss: 0.857608]\n",
            "[Epoch 8/200] [Batch 0/100] [D loss: 0.572636] [G loss: 0.936864]\n",
            "[Epoch 9/200] [Batch 0/100] [D loss: 0.577758] [G loss: 0.887962]\n",
            "[Epoch 10/200] [Batch 0/100] [D loss: 0.567548] [G loss: 0.918582]\n",
            "[Epoch 11/200] [Batch 0/100] [D loss: 0.606539] [G loss: 0.839366]\n",
            "[Epoch 12/200] [Batch 0/100] [D loss: 0.528844] [G loss: 1.035880]\n",
            "[Epoch 13/200] [Batch 0/100] [D loss: 0.589962] [G loss: 0.846491]\n",
            "[Epoch 14/200] [Batch 0/100] [D loss: 0.676576] [G loss: 0.724015]\n",
            "[Epoch 15/200] [Batch 0/100] [D loss: 0.647368] [G loss: 0.848165]\n",
            "[Epoch 16/200] [Batch 0/100] [D loss: 0.631899] [G loss: 0.783457]\n",
            "[Epoch 17/200] [Batch 0/100] [D loss: 0.613528] [G loss: 0.783514]\n",
            "[Epoch 18/200] [Batch 0/100] [D loss: 0.621814] [G loss: 0.824166]\n",
            "[Epoch 19/200] [Batch 0/100] [D loss: 0.602298] [G loss: 0.885290]\n",
            "[Epoch 20/200] [Batch 0/100] [D loss: 0.642354] [G loss: 0.837404]\n",
            "[Epoch 21/200] [Batch 0/100] [D loss: 0.664775] [G loss: 0.802195]\n",
            "[Epoch 22/200] [Batch 0/100] [D loss: 0.589834] [G loss: 1.019348]\n",
            "[Epoch 23/200] [Batch 0/100] [D loss: 0.648525] [G loss: 0.810165]\n",
            "[Epoch 24/200] [Batch 0/100] [D loss: 0.642332] [G loss: 0.815611]\n",
            "[Epoch 25/200] [Batch 0/100] [D loss: 0.627777] [G loss: 0.850568]\n",
            "[Epoch 26/200] [Batch 0/100] [D loss: 0.670335] [G loss: 0.671585]\n",
            "[Epoch 27/200] [Batch 0/100] [D loss: 0.668654] [G loss: 0.784810]\n",
            "[Epoch 28/200] [Batch 0/100] [D loss: 0.628112] [G loss: 0.822126]\n",
            "[Epoch 29/200] [Batch 0/100] [D loss: 0.618685] [G loss: 0.884108]\n",
            "[Epoch 30/200] [Batch 0/100] [D loss: 0.599710] [G loss: 0.816789]\n",
            "[Epoch 31/200] [Batch 0/100] [D loss: 0.613674] [G loss: 0.876776]\n",
            "[Epoch 32/200] [Batch 0/100] [D loss: 0.616932] [G loss: 0.837217]\n",
            "[Epoch 33/200] [Batch 0/100] [D loss: 0.651470] [G loss: 0.698850]\n",
            "[Epoch 34/200] [Batch 0/100] [D loss: 0.657340] [G loss: 0.857223]\n",
            "[Epoch 35/200] [Batch 0/100] [D loss: 0.599029] [G loss: 0.944644]\n",
            "[Epoch 36/200] [Batch 0/100] [D loss: 0.642160] [G loss: 0.828961]\n",
            "[Epoch 37/200] [Batch 0/100] [D loss: 0.633281] [G loss: 0.889058]\n",
            "[Epoch 38/200] [Batch 0/100] [D loss: 0.613771] [G loss: 0.806659]\n",
            "[Epoch 39/200] [Batch 0/100] [D loss: 0.598303] [G loss: 0.937446]\n",
            "[Epoch 40/200] [Batch 0/100] [D loss: 0.650266] [G loss: 0.793985]\n",
            "[Epoch 41/200] [Batch 0/100] [D loss: 0.614465] [G loss: 0.796950]\n",
            "[Epoch 42/200] [Batch 0/100] [D loss: 0.614433] [G loss: 0.896820]\n",
            "[Epoch 43/200] [Batch 0/100] [D loss: 0.641051] [G loss: 0.794326]\n",
            "[Epoch 44/200] [Batch 0/100] [D loss: 0.636693] [G loss: 0.837130]\n",
            "[Epoch 45/200] [Batch 0/100] [D loss: 0.606633] [G loss: 0.827603]\n",
            "[Epoch 46/200] [Batch 0/100] [D loss: 0.624472] [G loss: 0.879120]\n",
            "[Epoch 47/200] [Batch 0/100] [D loss: 0.626342] [G loss: 0.881748]\n",
            "[Epoch 48/200] [Batch 0/100] [D loss: 0.617896] [G loss: 0.828417]\n",
            "[Epoch 49/200] [Batch 0/100] [D loss: 0.675928] [G loss: 0.746014]\n",
            "[Epoch 50/200] [Batch 0/100] [D loss: 0.616987] [G loss: 0.872424]\n",
            "[Epoch 51/200] [Batch 0/100] [D loss: 0.655343] [G loss: 0.744514]\n",
            "[Epoch 52/200] [Batch 0/100] [D loss: 0.646442] [G loss: 0.834691]\n",
            "[Epoch 53/200] [Batch 0/100] [D loss: 0.664476] [G loss: 0.751324]\n",
            "[Epoch 54/200] [Batch 0/100] [D loss: 0.672273] [G loss: 0.817817]\n",
            "[Epoch 55/200] [Batch 0/100] [D loss: 0.682944] [G loss: 0.683210]\n",
            "[Epoch 56/200] [Batch 0/100] [D loss: 0.692150] [G loss: 0.724243]\n",
            "[Epoch 57/200] [Batch 0/100] [D loss: 0.644501] [G loss: 0.754122]\n",
            "[Epoch 58/200] [Batch 0/100] [D loss: 0.659901] [G loss: 0.735289]\n",
            "[Epoch 59/200] [Batch 0/100] [D loss: 0.680208] [G loss: 0.732630]\n",
            "[Epoch 60/200] [Batch 0/100] [D loss: 0.690897] [G loss: 0.676518]\n",
            "[Epoch 61/200] [Batch 0/100] [D loss: 0.670384] [G loss: 0.724199]\n",
            "[Epoch 62/200] [Batch 0/100] [D loss: 0.681418] [G loss: 0.709393]\n",
            "[Epoch 63/200] [Batch 0/100] [D loss: 0.695732] [G loss: 0.656840]\n",
            "[Epoch 64/200] [Batch 0/100] [D loss: 0.672001] [G loss: 0.726988]\n",
            "[Epoch 65/200] [Batch 0/100] [D loss: 0.679770] [G loss: 0.736481]\n",
            "[Epoch 66/200] [Batch 0/100] [D loss: 0.693653] [G loss: 0.737808]\n",
            "[Epoch 67/200] [Batch 0/100] [D loss: 0.681697] [G loss: 0.733359]\n",
            "[Epoch 68/200] [Batch 0/100] [D loss: 0.687881] [G loss: 0.698170]\n",
            "[Epoch 69/200] [Batch 0/100] [D loss: 0.696236] [G loss: 0.719118]\n",
            "[Epoch 70/200] [Batch 0/100] [D loss: 0.715966] [G loss: 0.681605]\n",
            "[Epoch 71/200] [Batch 0/100] [D loss: 0.687914] [G loss: 0.717833]\n",
            "[Epoch 72/200] [Batch 0/100] [D loss: 0.676123] [G loss: 0.735371]\n",
            "[Epoch 73/200] [Batch 0/100] [D loss: 0.686568] [G loss: 0.735697]\n",
            "[Epoch 74/200] [Batch 0/100] [D loss: 0.687049] [G loss: 0.701549]\n",
            "[Epoch 75/200] [Batch 0/100] [D loss: 0.696600] [G loss: 0.675795]\n",
            "[Epoch 76/200] [Batch 0/100] [D loss: 0.689255] [G loss: 0.684742]\n",
            "[Epoch 77/200] [Batch 0/100] [D loss: 0.681356] [G loss: 0.823582]\n",
            "[Epoch 78/200] [Batch 0/100] [D loss: 0.694269] [G loss: 0.712883]\n",
            "[Epoch 79/200] [Batch 0/100] [D loss: 0.691181] [G loss: 0.728741]\n",
            "[Epoch 80/200] [Batch 0/100] [D loss: 0.697659] [G loss: 0.689583]\n",
            "[Epoch 81/200] [Batch 0/100] [D loss: 0.692603] [G loss: 0.648693]\n",
            "[Epoch 82/200] [Batch 0/100] [D loss: 0.693488] [G loss: 0.707318]\n",
            "[Epoch 83/200] [Batch 0/100] [D loss: 0.692388] [G loss: 0.670730]\n",
            "[Epoch 84/200] [Batch 0/100] [D loss: 0.697404] [G loss: 0.643096]\n",
            "[Epoch 85/200] [Batch 0/100] [D loss: 0.690897] [G loss: 0.708685]\n",
            "[Epoch 86/200] [Batch 0/100] [D loss: 0.699425] [G loss: 0.707937]\n",
            "[Epoch 87/200] [Batch 0/100] [D loss: 0.701902] [G loss: 0.712499]\n",
            "[Epoch 88/200] [Batch 0/100] [D loss: 0.688916] [G loss: 0.738687]\n",
            "[Epoch 89/200] [Batch 0/100] [D loss: 0.693107] [G loss: 0.654415]\n",
            "[Epoch 90/200] [Batch 0/100] [D loss: 0.691041] [G loss: 0.722839]\n",
            "[Epoch 91/200] [Batch 0/100] [D loss: 0.688382] [G loss: 0.727663]\n",
            "[Epoch 92/200] [Batch 0/100] [D loss: 0.690613] [G loss: 0.738184]\n",
            "[Epoch 93/200] [Batch 0/100] [D loss: 0.685370] [G loss: 0.744543]\n",
            "[Epoch 94/200] [Batch 0/100] [D loss: 0.676853] [G loss: 0.713490]\n",
            "[Epoch 95/200] [Batch 0/100] [D loss: 0.681987] [G loss: 0.719551]\n",
            "[Epoch 96/200] [Batch 0/100] [D loss: 0.695442] [G loss: 0.662423]\n",
            "[Epoch 97/200] [Batch 0/100] [D loss: 0.686003] [G loss: 0.738028]\n",
            "[Epoch 98/200] [Batch 0/100] [D loss: 0.687783] [G loss: 0.782961]\n",
            "[Epoch 99/200] [Batch 0/100] [D loss: 0.689710] [G loss: 0.679889]\n",
            "[Epoch 100/200] [Batch 0/100] [D loss: 0.691913] [G loss: 0.735244]\n",
            "[Epoch 101/200] [Batch 0/100] [D loss: 0.704956] [G loss: 0.745750]\n",
            "[Epoch 102/200] [Batch 0/100] [D loss: 0.706290] [G loss: 0.712721]\n",
            "[Epoch 103/200] [Batch 0/100] [D loss: 0.695916] [G loss: 0.746163]\n",
            "[Epoch 104/200] [Batch 0/100] [D loss: 0.681564] [G loss: 0.673440]\n",
            "[Epoch 105/200] [Batch 0/100] [D loss: 0.693538] [G loss: 0.686271]\n",
            "[Epoch 106/200] [Batch 0/100] [D loss: 0.692906] [G loss: 0.679290]\n",
            "[Epoch 107/200] [Batch 0/100] [D loss: 0.689716] [G loss: 0.688064]\n",
            "[Epoch 108/200] [Batch 0/100] [D loss: 0.688210] [G loss: 0.733570]\n",
            "[Epoch 109/200] [Batch 0/100] [D loss: 0.682712] [G loss: 0.768659]\n",
            "[Epoch 110/200] [Batch 0/100] [D loss: 0.694420] [G loss: 0.736364]\n",
            "[Epoch 111/200] [Batch 0/100] [D loss: 0.696340] [G loss: 0.755560]\n",
            "[Epoch 112/200] [Batch 0/100] [D loss: 0.703612] [G loss: 0.673152]\n",
            "[Epoch 113/200] [Batch 0/100] [D loss: 0.696870] [G loss: 0.745475]\n",
            "[Epoch 114/200] [Batch 0/100] [D loss: 0.693930] [G loss: 0.793575]\n",
            "[Epoch 115/200] [Batch 0/100] [D loss: 0.686816] [G loss: 0.694240]\n",
            "[Epoch 116/200] [Batch 0/100] [D loss: 0.695891] [G loss: 0.762900]\n",
            "[Epoch 117/200] [Batch 0/100] [D loss: 0.694352] [G loss: 0.714933]\n",
            "[Epoch 118/200] [Batch 0/100] [D loss: 0.694589] [G loss: 0.675523]\n",
            "[Epoch 119/200] [Batch 0/100] [D loss: 0.688356] [G loss: 0.704700]\n",
            "[Epoch 120/200] [Batch 0/100] [D loss: 0.688586] [G loss: 0.684529]\n",
            "[Epoch 121/200] [Batch 0/100] [D loss: 0.683832] [G loss: 0.666420]\n",
            "[Epoch 122/200] [Batch 0/100] [D loss: 0.693827] [G loss: 0.746226]\n",
            "[Epoch 123/200] [Batch 0/100] [D loss: 0.696143] [G loss: 0.729280]\n",
            "[Epoch 124/200] [Batch 0/100] [D loss: 0.681000] [G loss: 0.761090]\n",
            "[Epoch 125/200] [Batch 0/100] [D loss: 0.692552] [G loss: 0.720461]\n",
            "[Epoch 126/200] [Batch 0/100] [D loss: 0.685229] [G loss: 0.710813]\n",
            "[Epoch 127/200] [Batch 0/100] [D loss: 0.701272] [G loss: 0.685033]\n",
            "[Epoch 128/200] [Batch 0/100] [D loss: 0.697317] [G loss: 0.716400]\n",
            "[Epoch 129/200] [Batch 0/100] [D loss: 0.696561] [G loss: 0.751189]\n",
            "[Epoch 130/200] [Batch 0/100] [D loss: 0.692986] [G loss: 0.712053]\n",
            "[Epoch 131/200] [Batch 0/100] [D loss: 0.698553] [G loss: 0.751767]\n",
            "[Epoch 132/200] [Batch 0/100] [D loss: 0.692271] [G loss: 0.714800]\n",
            "[Epoch 133/200] [Batch 0/100] [D loss: 0.672571] [G loss: 0.736396]\n",
            "[Epoch 134/200] [Batch 0/100] [D loss: 0.691225] [G loss: 0.724405]\n",
            "[Epoch 135/200] [Batch 0/100] [D loss: 0.692775] [G loss: 0.714242]\n",
            "[Epoch 136/200] [Batch 0/100] [D loss: 0.691302] [G loss: 0.689393]\n",
            "[Epoch 137/200] [Batch 0/100] [D loss: 0.690820] [G loss: 0.733153]\n",
            "[Epoch 138/200] [Batch 0/100] [D loss: 0.687482] [G loss: 0.736464]\n",
            "[Epoch 139/200] [Batch 0/100] [D loss: 0.689569] [G loss: 0.720043]\n",
            "[Epoch 140/200] [Batch 0/100] [D loss: 0.690522] [G loss: 0.647254]\n",
            "[Epoch 141/200] [Batch 0/100] [D loss: 0.701656] [G loss: 0.671045]\n",
            "[Epoch 142/200] [Batch 0/100] [D loss: 0.686998] [G loss: 0.726665]\n",
            "[Epoch 143/200] [Batch 0/100] [D loss: 0.683844] [G loss: 0.759639]\n",
            "[Epoch 144/200] [Batch 0/100] [D loss: 0.698608] [G loss: 0.665398]\n",
            "[Epoch 145/200] [Batch 0/100] [D loss: 0.687564] [G loss: 0.712540]\n",
            "[Epoch 146/200] [Batch 0/100] [D loss: 0.699499] [G loss: 0.636075]\n",
            "[Epoch 147/200] [Batch 0/100] [D loss: 0.692856] [G loss: 0.727101]\n",
            "[Epoch 148/200] [Batch 0/100] [D loss: 0.689938] [G loss: 0.659378]\n",
            "[Epoch 149/200] [Batch 0/100] [D loss: 0.690020] [G loss: 0.690090]\n",
            "[Epoch 150/200] [Batch 0/100] [D loss: 0.684426] [G loss: 0.790152]\n",
            "[Epoch 151/200] [Batch 0/100] [D loss: 0.691263] [G loss: 0.670514]\n",
            "[Epoch 152/200] [Batch 0/100] [D loss: 0.690843] [G loss: 0.669662]\n",
            "[Epoch 153/200] [Batch 0/100] [D loss: 0.695852] [G loss: 0.720101]\n",
            "[Epoch 154/200] [Batch 0/100] [D loss: 0.695915] [G loss: 0.694720]\n",
            "[Epoch 155/200] [Batch 0/100] [D loss: 0.687199] [G loss: 0.713314]\n",
            "[Epoch 156/200] [Batch 0/100] [D loss: 0.701639] [G loss: 0.717505]\n",
            "[Epoch 157/200] [Batch 0/100] [D loss: 0.693249] [G loss: 0.688178]\n",
            "[Epoch 158/200] [Batch 0/100] [D loss: 0.695483] [G loss: 0.682626]\n",
            "[Epoch 159/200] [Batch 0/100] [D loss: 0.694574] [G loss: 0.683361]\n",
            "[Epoch 160/200] [Batch 0/100] [D loss: 0.696947] [G loss: 0.658580]\n",
            "[Epoch 161/200] [Batch 0/100] [D loss: 0.690164] [G loss: 0.722781]\n",
            "[Epoch 162/200] [Batch 0/100] [D loss: 0.688654] [G loss: 0.682391]\n",
            "[Epoch 163/200] [Batch 0/100] [D loss: 0.685817] [G loss: 0.678790]\n",
            "[Epoch 164/200] [Batch 0/100] [D loss: 0.690937] [G loss: 0.805145]\n",
            "[Epoch 165/200] [Batch 0/100] [D loss: 0.699030] [G loss: 0.645160]\n",
            "[Epoch 166/200] [Batch 0/100] [D loss: 0.692597] [G loss: 0.734424]\n",
            "[Epoch 167/200] [Batch 0/100] [D loss: 0.692936] [G loss: 0.697610]\n",
            "[Epoch 168/200] [Batch 0/100] [D loss: 0.695228] [G loss: 0.705959]\n",
            "[Epoch 169/200] [Batch 0/100] [D loss: 0.693046] [G loss: 0.692446]\n",
            "[Epoch 170/200] [Batch 0/100] [D loss: 0.693697] [G loss: 0.734071]\n",
            "[Epoch 171/200] [Batch 0/100] [D loss: 0.693564] [G loss: 0.658116]\n",
            "[Epoch 172/200] [Batch 0/100] [D loss: 0.704635] [G loss: 0.765798]\n",
            "[Epoch 173/200] [Batch 0/100] [D loss: 0.686088] [G loss: 0.696590]\n",
            "[Epoch 174/200] [Batch 0/100] [D loss: 0.692508] [G loss: 0.755680]\n",
            "[Epoch 175/200] [Batch 0/100] [D loss: 0.693008] [G loss: 0.689724]\n",
            "[Epoch 176/200] [Batch 0/100] [D loss: 0.693756] [G loss: 0.652326]\n",
            "[Epoch 177/200] [Batch 0/100] [D loss: 0.702840] [G loss: 0.728854]\n",
            "[Epoch 178/200] [Batch 0/100] [D loss: 0.689302] [G loss: 0.689914]\n",
            "[Epoch 179/200] [Batch 0/100] [D loss: 0.695860] [G loss: 0.684336]\n",
            "[Epoch 180/200] [Batch 0/100] [D loss: 0.691144] [G loss: 0.698045]\n",
            "[Epoch 181/200] [Batch 0/100] [D loss: 0.693011] [G loss: 0.760439]\n",
            "[Epoch 182/200] [Batch 0/100] [D loss: 0.687830] [G loss: 0.686798]\n",
            "[Epoch 183/200] [Batch 0/100] [D loss: 0.695922] [G loss: 0.714518]\n",
            "[Epoch 184/200] [Batch 0/100] [D loss: 0.691447] [G loss: 0.717137]\n",
            "[Epoch 185/200] [Batch 0/100] [D loss: 0.688625] [G loss: 0.648410]\n",
            "[Epoch 186/200] [Batch 0/100] [D loss: 0.689420] [G loss: 0.707829]\n",
            "[Epoch 187/200] [Batch 0/100] [D loss: 0.685763] [G loss: 0.711876]\n",
            "[Epoch 188/200] [Batch 0/100] [D loss: 0.690852] [G loss: 0.678914]\n",
            "[Epoch 189/200] [Batch 0/100] [D loss: 0.687863] [G loss: 0.709590]\n",
            "[Epoch 190/200] [Batch 0/100] [D loss: 0.694594] [G loss: 0.659431]\n",
            "[Epoch 191/200] [Batch 0/100] [D loss: 0.688494] [G loss: 0.658302]\n",
            "[Epoch 192/200] [Batch 0/100] [D loss: 0.694526] [G loss: 0.720757]\n",
            "[Epoch 193/200] [Batch 0/100] [D loss: 0.684931] [G loss: 0.692289]\n",
            "[Epoch 194/200] [Batch 0/100] [D loss: 0.689757] [G loss: 0.760412]\n",
            "[Epoch 195/200] [Batch 0/100] [D loss: 0.692712] [G loss: 0.637007]\n",
            "[Epoch 196/200] [Batch 0/100] [D loss: 0.693704] [G loss: 0.707271]\n",
            "[Epoch 197/200] [Batch 0/100] [D loss: 0.690734] [G loss: 0.714703]\n",
            "[Epoch 198/200] [Batch 0/100] [D loss: 0.689636] [G loss: 0.677480]\n",
            "[Epoch 199/200] [Batch 0/100] [D loss: 0.694198] [G loss: 0.702354]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Visualization**"
      ],
      "metadata": {
        "id": "NZETyUC0v3cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load generated samples\n",
        "generated_samples = []\n",
        "for epoch in range(0, n_epochs, 10):\n",
        "    samples = np.load(f\"samples/epoch_{epoch}.npy\")\n",
        "    generated_samples.extend(samples)\n",
        "\n",
        "generated_samples = np.concatenate(generated_samples)\n",
        "\n",
        "\n",
        "\n",
        "# Plot histogram\n",
        "plt.hist(generated_samples, bins=50, density=True)\n",
        "plt.xlabel(\"Generated Samples\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.title(\"Histogram of Generated Samples\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "gU0bYVkEMMOC",
        "outputId": "4b5c0425-9262-48f7-9ef4-b06743b1d4cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Implementing to sampling from 2D Gaussian**\n"
      ],
      "metadata": {
        "id": "tsdgaChfJzmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "os.makedirs(\"samples\", exist_ok=True)\n",
        "\n",
        "n_dimensions = 2\n",
        "n_epochs = 200\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "n_cpu = 8\n",
        "latent_dim = 100\n",
        "sample_interval = 400\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, n_dimensions)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.model(z)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(n_dimensions, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        validity = self.model(x)\n",
        "        return validity\n",
        "\n",
        "\n",
        "class Bimodal_ND():\n",
        "  # sample from a multivariate bimodal mixture of gaussians\n",
        "  def __init__(self, m1, s1, m2, s2, w1, n_dim):\n",
        "    self.m1 = m1\n",
        "    self.m2 = m2\n",
        "    self.s1 = s1\n",
        "    self.s2 = s2\n",
        "    self.w1 = w1\n",
        "    self.n_dim = n_dim\n",
        "\n",
        "  def sample(self, n_samples):\n",
        "    mode = np.random.choice([0, 1], size=n_samples, p=[self.w1, 1 - self.w1])\n",
        "    samples1 = np.random.multivariate_normal(self.m1, self.s1, size=np.sum(mode == 0))\n",
        "    samples2 = np.random.multivariate_normal(self.m2, self.s2, size=np.sum(mode == 1))\n",
        "\n",
        "    samples = np.concatenate([samples1, samples2])\n",
        "    samples = np.reshape(samples, (n_samples, self.n_dim))\n",
        "\n",
        "    return samples\n",
        "\n",
        "\n",
        "# Loss function\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "\n",
        "\n",
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "m1 = np.array([2,2])\n",
        "s1 = np.array([[1,0],[0,1]])\n",
        "m2 = np.array([-2,-2])\n",
        "s2 = np.array([[1,0],[0,1]])\n",
        "w1 = 0.5\n",
        "\n",
        "# # of iterations per epoch\n",
        "num_iters = 200\n",
        "\n",
        "\n",
        "distr = Bimodal_ND(m1, s1, m2, s2, w1, n_dimensions)\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
        "        fake = Variable(Tensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
        "\n",
        "        # Generate a batch of samples\n",
        "        gen_samples = generator(z)\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Measure discriminator's ability to distinguish real samples)\n",
        "\n",
        "        data = distr.sample(batch_size)\n",
        "\n",
        "        real_samples = Variable(Tensor(data))\n",
        "        real_loss = adversarial_loss(discriminator(real_samples), valid)\n",
        "\n",
        "        # Measure discriminator's ability to distinguish generated samples\n",
        "        fake_loss = adversarial_loss(discriminator(gen_samples.detach()), fake)\n",
        "\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        g_loss = adversarial_loss(discriminator(gen_samples), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        if i % num_iters == 0:\n",
        "            print(\n",
        "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "                % (epoch, n_epochs, i, num_iters, d_loss.item(), g_loss.item())\n",
        "            )\n",
        "\n",
        "    # Generate and save samples\n",
        "    if epoch % 10 == 0:\n",
        "        with torch.no_grad():\n",
        "            z = Variable(Tensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
        "            gen_samples = generator(z)\n",
        "            np.save(f\"samples/epoch_{epoch}.npy\", gen_samples.cpu().numpy())\n"
      ],
      "metadata": {
        "id": "CGxBHpe6KE-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e075a9b-ae54-471f-a94d-c4fc217c5986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0/200] [Batch 0/200] [D loss: 0.657999] [G loss: 0.631571]\n",
            "[Epoch 1/200] [Batch 0/200] [D loss: 0.652094] [G loss: 1.142488]\n",
            "[Epoch 2/200] [Batch 0/200] [D loss: 0.621630] [G loss: 1.074748]\n",
            "[Epoch 3/200] [Batch 0/200] [D loss: 0.500049] [G loss: 1.013386]\n",
            "[Epoch 4/200] [Batch 0/200] [D loss: 0.652495] [G loss: 0.752813]\n",
            "[Epoch 5/200] [Batch 0/200] [D loss: 0.640955] [G loss: 0.813864]\n",
            "[Epoch 6/200] [Batch 0/200] [D loss: 0.645561] [G loss: 0.766172]\n",
            "[Epoch 7/200] [Batch 0/200] [D loss: 0.642585] [G loss: 0.817470]\n",
            "[Epoch 8/200] [Batch 0/200] [D loss: 0.658210] [G loss: 0.818089]\n",
            "[Epoch 9/200] [Batch 0/200] [D loss: 0.640370] [G loss: 0.834077]\n",
            "[Epoch 10/200] [Batch 0/200] [D loss: 0.653714] [G loss: 0.838459]\n",
            "[Epoch 11/200] [Batch 0/200] [D loss: 0.712069] [G loss: 0.728581]\n",
            "[Epoch 12/200] [Batch 0/200] [D loss: 0.702179] [G loss: 0.756484]\n",
            "[Epoch 13/200] [Batch 0/200] [D loss: 0.680702] [G loss: 0.732418]\n",
            "[Epoch 14/200] [Batch 0/200] [D loss: 0.699864] [G loss: 0.689912]\n",
            "[Epoch 15/200] [Batch 0/200] [D loss: 0.697549] [G loss: 0.732088]\n",
            "[Epoch 16/200] [Batch 0/200] [D loss: 0.694172] [G loss: 0.745097]\n",
            "[Epoch 17/200] [Batch 0/200] [D loss: 0.712010] [G loss: 0.707654]\n",
            "[Epoch 18/200] [Batch 0/200] [D loss: 0.700187] [G loss: 0.733512]\n",
            "[Epoch 19/200] [Batch 0/200] [D loss: 0.688235] [G loss: 0.690485]\n",
            "[Epoch 20/200] [Batch 0/200] [D loss: 0.708211] [G loss: 0.649363]\n",
            "[Epoch 21/200] [Batch 0/200] [D loss: 0.677183] [G loss: 0.693522]\n",
            "[Epoch 22/200] [Batch 0/200] [D loss: 0.692676] [G loss: 0.716685]\n",
            "[Epoch 23/200] [Batch 0/200] [D loss: 0.696825] [G loss: 0.662636]\n",
            "[Epoch 24/200] [Batch 0/200] [D loss: 0.686273] [G loss: 0.725103]\n",
            "[Epoch 25/200] [Batch 0/200] [D loss: 0.689674] [G loss: 0.734636]\n",
            "[Epoch 26/200] [Batch 0/200] [D loss: 0.684933] [G loss: 0.699567]\n",
            "[Epoch 27/200] [Batch 0/200] [D loss: 0.670032] [G loss: 0.723221]\n",
            "[Epoch 28/200] [Batch 0/200] [D loss: 0.688278] [G loss: 0.714025]\n",
            "[Epoch 29/200] [Batch 0/200] [D loss: 0.689595] [G loss: 0.680102]\n",
            "[Epoch 30/200] [Batch 0/200] [D loss: 0.702919] [G loss: 0.691483]\n",
            "[Epoch 31/200] [Batch 0/200] [D loss: 0.712845] [G loss: 0.697625]\n",
            "[Epoch 32/200] [Batch 0/200] [D loss: 0.679389] [G loss: 0.791741]\n",
            "[Epoch 33/200] [Batch 0/200] [D loss: 0.677167] [G loss: 0.711296]\n",
            "[Epoch 34/200] [Batch 0/200] [D loss: 0.692259] [G loss: 0.677710]\n",
            "[Epoch 35/200] [Batch 0/200] [D loss: 0.696186] [G loss: 0.764387]\n",
            "[Epoch 36/200] [Batch 0/200] [D loss: 0.667826] [G loss: 0.779612]\n",
            "[Epoch 37/200] [Batch 0/200] [D loss: 0.668813] [G loss: 0.722984]\n",
            "[Epoch 38/200] [Batch 0/200] [D loss: 0.692006] [G loss: 0.766436]\n",
            "[Epoch 39/200] [Batch 0/200] [D loss: 0.692764] [G loss: 0.702668]\n",
            "[Epoch 40/200] [Batch 0/200] [D loss: 0.707337] [G loss: 0.632730]\n",
            "[Epoch 41/200] [Batch 0/200] [D loss: 0.702787] [G loss: 0.752578]\n",
            "[Epoch 42/200] [Batch 0/200] [D loss: 0.705719] [G loss: 0.701132]\n",
            "[Epoch 43/200] [Batch 0/200] [D loss: 0.670921] [G loss: 0.745909]\n",
            "[Epoch 44/200] [Batch 0/200] [D loss: 0.689642] [G loss: 0.716743]\n",
            "[Epoch 45/200] [Batch 0/200] [D loss: 0.692984] [G loss: 0.682190]\n",
            "[Epoch 46/200] [Batch 0/200] [D loss: 0.717486] [G loss: 0.679288]\n",
            "[Epoch 47/200] [Batch 0/200] [D loss: 0.688436] [G loss: 0.672269]\n",
            "[Epoch 48/200] [Batch 0/200] [D loss: 0.682808] [G loss: 0.698873]\n",
            "[Epoch 49/200] [Batch 0/200] [D loss: 0.723122] [G loss: 0.660300]\n",
            "[Epoch 50/200] [Batch 0/200] [D loss: 0.680157] [G loss: 0.797946]\n",
            "[Epoch 51/200] [Batch 0/200] [D loss: 0.710621] [G loss: 0.661331]\n",
            "[Epoch 52/200] [Batch 0/200] [D loss: 0.703572] [G loss: 0.701162]\n",
            "[Epoch 53/200] [Batch 0/200] [D loss: 0.697507] [G loss: 0.691329]\n",
            "[Epoch 54/200] [Batch 0/200] [D loss: 0.677040] [G loss: 0.749833]\n",
            "[Epoch 55/200] [Batch 0/200] [D loss: 0.685869] [G loss: 0.703570]\n",
            "[Epoch 56/200] [Batch 0/200] [D loss: 0.706808] [G loss: 0.680590]\n",
            "[Epoch 57/200] [Batch 0/200] [D loss: 0.701565] [G loss: 0.708483]\n",
            "[Epoch 58/200] [Batch 0/200] [D loss: 0.690553] [G loss: 0.721356]\n",
            "[Epoch 59/200] [Batch 0/200] [D loss: 0.704704] [G loss: 0.714626]\n",
            "[Epoch 60/200] [Batch 0/200] [D loss: 0.696663] [G loss: 0.681531]\n",
            "[Epoch 61/200] [Batch 0/200] [D loss: 0.698346] [G loss: 0.695880]\n",
            "[Epoch 62/200] [Batch 0/200] [D loss: 0.694989] [G loss: 0.681741]\n",
            "[Epoch 63/200] [Batch 0/200] [D loss: 0.684204] [G loss: 0.701961]\n",
            "[Epoch 64/200] [Batch 0/200] [D loss: 0.698966] [G loss: 0.711458]\n",
            "[Epoch 65/200] [Batch 0/200] [D loss: 0.679401] [G loss: 0.704311]\n",
            "[Epoch 66/200] [Batch 0/200] [D loss: 0.683491] [G loss: 0.717228]\n",
            "[Epoch 67/200] [Batch 0/200] [D loss: 0.684486] [G loss: 0.689175]\n",
            "[Epoch 68/200] [Batch 0/200] [D loss: 0.695999] [G loss: 0.720044]\n",
            "[Epoch 69/200] [Batch 0/200] [D loss: 0.687711] [G loss: 0.770888]\n",
            "[Epoch 70/200] [Batch 0/200] [D loss: 0.697144] [G loss: 0.690545]\n",
            "[Epoch 71/200] [Batch 0/200] [D loss: 0.688869] [G loss: 0.700367]\n",
            "[Epoch 72/200] [Batch 0/200] [D loss: 0.689182] [G loss: 0.708492]\n",
            "[Epoch 73/200] [Batch 0/200] [D loss: 0.696653] [G loss: 0.690298]\n",
            "[Epoch 74/200] [Batch 0/200] [D loss: 0.706337] [G loss: 0.669653]\n",
            "[Epoch 75/200] [Batch 0/200] [D loss: 0.696012] [G loss: 0.724354]\n",
            "[Epoch 76/200] [Batch 0/200] [D loss: 0.714257] [G loss: 0.682713]\n",
            "[Epoch 77/200] [Batch 0/200] [D loss: 0.709900] [G loss: 0.707513]\n",
            "[Epoch 78/200] [Batch 0/200] [D loss: 0.697037] [G loss: 0.666269]\n",
            "[Epoch 79/200] [Batch 0/200] [D loss: 0.692125] [G loss: 0.721573]\n",
            "[Epoch 80/200] [Batch 0/200] [D loss: 0.704279] [G loss: 0.697546]\n",
            "[Epoch 81/200] [Batch 0/200] [D loss: 0.692521] [G loss: 0.675826]\n",
            "[Epoch 82/200] [Batch 0/200] [D loss: 0.692914] [G loss: 0.666947]\n",
            "[Epoch 83/200] [Batch 0/200] [D loss: 0.696246] [G loss: 0.651217]\n",
            "[Epoch 84/200] [Batch 0/200] [D loss: 0.695026] [G loss: 0.697949]\n",
            "[Epoch 85/200] [Batch 0/200] [D loss: 0.700071] [G loss: 0.666123]\n",
            "[Epoch 86/200] [Batch 0/200] [D loss: 0.699929] [G loss: 0.683255]\n",
            "[Epoch 87/200] [Batch 0/200] [D loss: 0.693432] [G loss: 0.712767]\n",
            "[Epoch 88/200] [Batch 0/200] [D loss: 0.693449] [G loss: 0.670510]\n",
            "[Epoch 89/200] [Batch 0/200] [D loss: 0.688971] [G loss: 0.720625]\n",
            "[Epoch 90/200] [Batch 0/200] [D loss: 0.702801] [G loss: 0.689181]\n",
            "[Epoch 91/200] [Batch 0/200] [D loss: 0.689351] [G loss: 0.692576]\n",
            "[Epoch 92/200] [Batch 0/200] [D loss: 0.698011] [G loss: 0.718599]\n",
            "[Epoch 93/200] [Batch 0/200] [D loss: 0.693093] [G loss: 0.686747]\n",
            "[Epoch 94/200] [Batch 0/200] [D loss: 0.695068] [G loss: 0.694142]\n",
            "[Epoch 95/200] [Batch 0/200] [D loss: 0.695083] [G loss: 0.678859]\n",
            "[Epoch 96/200] [Batch 0/200] [D loss: 0.691548] [G loss: 0.688505]\n",
            "[Epoch 97/200] [Batch 0/200] [D loss: 0.696294] [G loss: 0.669906]\n",
            "[Epoch 98/200] [Batch 0/200] [D loss: 0.691596] [G loss: 0.709211]\n",
            "[Epoch 99/200] [Batch 0/200] [D loss: 0.685006] [G loss: 0.663070]\n",
            "[Epoch 100/200] [Batch 0/200] [D loss: 0.690094] [G loss: 0.719239]\n",
            "[Epoch 101/200] [Batch 0/200] [D loss: 0.692773] [G loss: 0.693788]\n",
            "[Epoch 102/200] [Batch 0/200] [D loss: 0.689536] [G loss: 0.697985]\n",
            "[Epoch 103/200] [Batch 0/200] [D loss: 0.692702] [G loss: 0.707542]\n",
            "[Epoch 104/200] [Batch 0/200] [D loss: 0.693351] [G loss: 0.674285]\n",
            "[Epoch 105/200] [Batch 0/200] [D loss: 0.693222] [G loss: 0.688002]\n",
            "[Epoch 106/200] [Batch 0/200] [D loss: 0.694981] [G loss: 0.699360]\n",
            "[Epoch 107/200] [Batch 0/200] [D loss: 0.693857] [G loss: 0.709046]\n",
            "[Epoch 108/200] [Batch 0/200] [D loss: 0.692710] [G loss: 0.682352]\n",
            "[Epoch 109/200] [Batch 0/200] [D loss: 0.695017] [G loss: 0.706474]\n",
            "[Epoch 110/200] [Batch 0/200] [D loss: 0.689479] [G loss: 0.699709]\n",
            "[Epoch 111/200] [Batch 0/200] [D loss: 0.693493] [G loss: 0.720192]\n",
            "[Epoch 112/200] [Batch 0/200] [D loss: 0.686697] [G loss: 0.696170]\n",
            "[Epoch 113/200] [Batch 0/200] [D loss: 0.692580] [G loss: 0.728097]\n",
            "[Epoch 114/200] [Batch 0/200] [D loss: 0.690044] [G loss: 0.679186]\n",
            "[Epoch 115/200] [Batch 0/200] [D loss: 0.692087] [G loss: 0.701318]\n",
            "[Epoch 116/200] [Batch 0/200] [D loss: 0.691653] [G loss: 0.693762]\n",
            "[Epoch 117/200] [Batch 0/200] [D loss: 0.692894] [G loss: 0.673346]\n",
            "[Epoch 118/200] [Batch 0/200] [D loss: 0.695490] [G loss: 0.703010]\n",
            "[Epoch 119/200] [Batch 0/200] [D loss: 0.694775] [G loss: 0.692005]\n",
            "[Epoch 120/200] [Batch 0/200] [D loss: 0.695961] [G loss: 0.687487]\n",
            "[Epoch 121/200] [Batch 0/200] [D loss: 0.692799] [G loss: 0.686336]\n",
            "[Epoch 122/200] [Batch 0/200] [D loss: 0.692463] [G loss: 0.713689]\n",
            "[Epoch 123/200] [Batch 0/200] [D loss: 0.689042] [G loss: 0.701473]\n",
            "[Epoch 124/200] [Batch 0/200] [D loss: 0.688769] [G loss: 0.687176]\n",
            "[Epoch 125/200] [Batch 0/200] [D loss: 0.688828] [G loss: 0.717963]\n",
            "[Epoch 126/200] [Batch 0/200] [D loss: 0.693820] [G loss: 0.683798]\n",
            "[Epoch 127/200] [Batch 0/200] [D loss: 0.689159] [G loss: 0.726982]\n",
            "[Epoch 128/200] [Batch 0/200] [D loss: 0.690073] [G loss: 0.736201]\n",
            "[Epoch 129/200] [Batch 0/200] [D loss: 0.691959] [G loss: 0.703946]\n",
            "[Epoch 130/200] [Batch 0/200] [D loss: 0.692853] [G loss: 0.707951]\n",
            "[Epoch 131/200] [Batch 0/200] [D loss: 0.692697] [G loss: 0.676430]\n",
            "[Epoch 132/200] [Batch 0/200] [D loss: 0.691742] [G loss: 0.716588]\n",
            "[Epoch 133/200] [Batch 0/200] [D loss: 0.685805] [G loss: 0.695818]\n",
            "[Epoch 134/200] [Batch 0/200] [D loss: 0.693682] [G loss: 0.687928]\n",
            "[Epoch 135/200] [Batch 0/200] [D loss: 0.693292] [G loss: 0.722469]\n",
            "[Epoch 136/200] [Batch 0/200] [D loss: 0.689719] [G loss: 0.687150]\n",
            "[Epoch 137/200] [Batch 0/200] [D loss: 0.695614] [G loss: 0.665822]\n",
            "[Epoch 138/200] [Batch 0/200] [D loss: 0.697084] [G loss: 0.698933]\n",
            "[Epoch 139/200] [Batch 0/200] [D loss: 0.698007] [G loss: 0.688108]\n",
            "[Epoch 140/200] [Batch 0/200] [D loss: 0.691263] [G loss: 0.693337]\n",
            "[Epoch 141/200] [Batch 0/200] [D loss: 0.694054] [G loss: 0.674808]\n",
            "[Epoch 142/200] [Batch 0/200] [D loss: 0.692727] [G loss: 0.699659]\n",
            "[Epoch 143/200] [Batch 0/200] [D loss: 0.693024] [G loss: 0.688932]\n",
            "[Epoch 144/200] [Batch 0/200] [D loss: 0.693710] [G loss: 0.678401]\n",
            "[Epoch 145/200] [Batch 0/200] [D loss: 0.700048] [G loss: 0.682705]\n",
            "[Epoch 146/200] [Batch 0/200] [D loss: 0.689744] [G loss: 0.697745]\n",
            "[Epoch 147/200] [Batch 0/200] [D loss: 0.696804] [G loss: 0.687856]\n",
            "[Epoch 148/200] [Batch 0/200] [D loss: 0.690007] [G loss: 0.710108]\n",
            "[Epoch 149/200] [Batch 0/200] [D loss: 0.694472] [G loss: 0.669230]\n",
            "[Epoch 150/200] [Batch 0/200] [D loss: 0.693691] [G loss: 0.708930]\n",
            "[Epoch 151/200] [Batch 0/200] [D loss: 0.685228] [G loss: 0.715343]\n",
            "[Epoch 152/200] [Batch 0/200] [D loss: 0.688634] [G loss: 0.714486]\n",
            "[Epoch 153/200] [Batch 0/200] [D loss: 0.694546] [G loss: 0.685327]\n",
            "[Epoch 154/200] [Batch 0/200] [D loss: 0.690703] [G loss: 0.698739]\n",
            "[Epoch 155/200] [Batch 0/200] [D loss: 0.685179] [G loss: 0.677818]\n",
            "[Epoch 156/200] [Batch 0/200] [D loss: 0.689198] [G loss: 0.706000]\n",
            "[Epoch 157/200] [Batch 0/200] [D loss: 0.691275] [G loss: 0.701016]\n",
            "[Epoch 158/200] [Batch 0/200] [D loss: 0.692631] [G loss: 0.705789]\n",
            "[Epoch 159/200] [Batch 0/200] [D loss: 0.693944] [G loss: 0.682351]\n",
            "[Epoch 160/200] [Batch 0/200] [D loss: 0.690801] [G loss: 0.695270]\n",
            "[Epoch 161/200] [Batch 0/200] [D loss: 0.694468] [G loss: 0.688004]\n",
            "[Epoch 162/200] [Batch 0/200] [D loss: 0.700967] [G loss: 0.675568]\n",
            "[Epoch 163/200] [Batch 0/200] [D loss: 0.693079] [G loss: 0.708137]\n",
            "[Epoch 164/200] [Batch 0/200] [D loss: 0.693712] [G loss: 0.690849]\n",
            "[Epoch 165/200] [Batch 0/200] [D loss: 0.692342] [G loss: 0.702631]\n",
            "[Epoch 166/200] [Batch 0/200] [D loss: 0.694614] [G loss: 0.699130]\n",
            "[Epoch 167/200] [Batch 0/200] [D loss: 0.696054] [G loss: 0.680892]\n",
            "[Epoch 168/200] [Batch 0/200] [D loss: 0.691929] [G loss: 0.690387]\n",
            "[Epoch 169/200] [Batch 0/200] [D loss: 0.693711] [G loss: 0.699461]\n",
            "[Epoch 170/200] [Batch 0/200] [D loss: 0.694079] [G loss: 0.683590]\n",
            "[Epoch 171/200] [Batch 0/200] [D loss: 0.693670] [G loss: 0.681212]\n",
            "[Epoch 172/200] [Batch 0/200] [D loss: 0.689415] [G loss: 0.684821]\n",
            "[Epoch 173/200] [Batch 0/200] [D loss: 0.691816] [G loss: 0.686861]\n",
            "[Epoch 174/200] [Batch 0/200] [D loss: 0.690659] [G loss: 0.693460]\n",
            "[Epoch 175/200] [Batch 0/200] [D loss: 0.692177] [G loss: 0.687442]\n",
            "[Epoch 176/200] [Batch 0/200] [D loss: 0.690605] [G loss: 0.707455]\n",
            "[Epoch 177/200] [Batch 0/200] [D loss: 0.691431] [G loss: 0.693886]\n",
            "[Epoch 178/200] [Batch 0/200] [D loss: 0.695040] [G loss: 0.685909]\n",
            "[Epoch 179/200] [Batch 0/200] [D loss: 0.695671] [G loss: 0.675704]\n",
            "[Epoch 180/200] [Batch 0/200] [D loss: 0.691569] [G loss: 0.693981]\n",
            "[Epoch 181/200] [Batch 0/200] [D loss: 0.693761] [G loss: 0.703875]\n",
            "[Epoch 182/200] [Batch 0/200] [D loss: 0.690563] [G loss: 0.692465]\n",
            "[Epoch 183/200] [Batch 0/200] [D loss: 0.693859] [G loss: 0.698216]\n",
            "[Epoch 184/200] [Batch 0/200] [D loss: 0.695176] [G loss: 0.684889]\n",
            "[Epoch 185/200] [Batch 0/200] [D loss: 0.693324] [G loss: 0.685185]\n",
            "[Epoch 186/200] [Batch 0/200] [D loss: 0.692674] [G loss: 0.704341]\n",
            "[Epoch 187/200] [Batch 0/200] [D loss: 0.694505] [G loss: 0.697328]\n",
            "[Epoch 188/200] [Batch 0/200] [D loss: 0.692642] [G loss: 0.694889]\n",
            "[Epoch 189/200] [Batch 0/200] [D loss: 0.696196] [G loss: 0.699435]\n",
            "[Epoch 190/200] [Batch 0/200] [D loss: 0.694780] [G loss: 0.694389]\n",
            "[Epoch 191/200] [Batch 0/200] [D loss: 0.695096] [G loss: 0.698884]\n",
            "[Epoch 192/200] [Batch 0/200] [D loss: 0.695245] [G loss: 0.699079]\n",
            "[Epoch 193/200] [Batch 0/200] [D loss: 0.694640] [G loss: 0.688647]\n",
            "[Epoch 194/200] [Batch 0/200] [D loss: 0.694305] [G loss: 0.689955]\n",
            "[Epoch 195/200] [Batch 0/200] [D loss: 0.693736] [G loss: 0.685509]\n",
            "[Epoch 196/200] [Batch 0/200] [D loss: 0.692679] [G loss: 0.692261]\n",
            "[Epoch 197/200] [Batch 0/200] [D loss: 0.691819] [G loss: 0.698551]\n",
            "[Epoch 198/200] [Batch 0/200] [D loss: 0.692461] [G loss: 0.694484]\n",
            "[Epoch 199/200] [Batch 0/200] [D loss: 0.694180] [G loss: 0.692389]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Visualizing 2D Bimodal**"
      ],
      "metadata": {
        "id": "-fS5HQ45U1JV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "generated_samples = []\n",
        "for epoch in range(0, n_epochs, 10):\n",
        "    samples = np.load(f\"samples/epoch_{epoch}.npy\")\n",
        "    generated_samples.append(samples)\n",
        "\n",
        "final = np.concatenate(generated_samples)\n",
        "\n",
        "plt.hist2d(final[:, 0], final[:, 1], bins = 100)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "djwhfnrIUz00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "0ec2dccc-fa20-4537-b95d-f40226fdb613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e4phO4ZQkaLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cadb07b-c9db-4daa-b03f-9e0a61faa969"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEWX/1IdttfOsHsqm/mW3r",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}